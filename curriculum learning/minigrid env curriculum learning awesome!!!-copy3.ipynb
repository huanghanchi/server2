{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import gym\n",
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "\n",
    "# Size in pixels of a cell in the full-scale human view\n",
    "CELL_PIXELS = 32\n",
    "\n",
    "# Number of cells (width and height) in the agent view\n",
    "AGENT_VIEW_SIZE = 7\n",
    "\n",
    "# Size of the array given as an observation to the agent\n",
    "OBS_ARRAY_SIZE = (AGENT_VIEW_SIZE, AGENT_VIEW_SIZE, 3)\n",
    "\n",
    "# Map of color names to RGB values\n",
    "COLORS = {\n",
    "    'red': np.array([255, 0, 0]),\n",
    "    'green': np.array([0, 255, 0]),\n",
    "    'blue': np.array([0, 0, 255]),\n",
    "    'purple': np.array([112, 39, 195]),\n",
    "    'yellow': np.array([255, 255, 0]),\n",
    "    'grey': np.array([100, 100, 100])\n",
    "}\n",
    "\n",
    "COLOR_NAMES = sorted(list(COLORS.keys()))\n",
    "\n",
    "# Used to map colors to integers\n",
    "COLOR_TO_IDX = {\n",
    "    'red': 0,\n",
    "    'green': 1,\n",
    "    'blue': 2,\n",
    "    'purple': 3,\n",
    "    'yellow': 4,\n",
    "    'grey': 5\n",
    "}\n",
    "\n",
    "IDX_TO_COLOR = dict(zip(COLOR_TO_IDX.values(), COLOR_TO_IDX.keys()))\n",
    "\n",
    "# Map of object type to integers\n",
    "OBJECT_TO_IDX = {\n",
    "    'empty': 0,\n",
    "    'wall': 1,\n",
    "    'floor': 2,\n",
    "    'door': 3,\n",
    "    'locked_door': 4,\n",
    "    'key': 5,\n",
    "    'ball': 6,\n",
    "    'box': 7,\n",
    "    'goal': 8\n",
    "}\n",
    "\n",
    "IDX_TO_OBJECT = dict(zip(OBJECT_TO_IDX.values(), OBJECT_TO_IDX.keys()))\n",
    "\n",
    "# Map of agent direction indices to vectors\n",
    "DIR_TO_VEC = [\n",
    "    # Pointing right (positive X)\n",
    "    np.array((1, 0)),\n",
    "    # Down (positive Y)\n",
    "    np.array((0, 1)),\n",
    "    # Pointing left (negative X)\n",
    "    np.array((-1, 0)),\n",
    "    # Up (negative Y)\n",
    "    np.array((0, -1)),\n",
    "]\n",
    "\n",
    "\n",
    "class WorldObj:\n",
    "    \"\"\"\n",
    "    Base class for grid world objects\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, type, color):\n",
    "   #     assert type in OBJECT_TO_IDX, type\n",
    "    #    assert color in COLOR_TO_IDX, color\n",
    "        self.type = type\n",
    "        self.color = color\n",
    "        self.contains = None\n",
    "\n",
    "        # Initial position of the object\n",
    "        self.init_pos = None\n",
    "\n",
    "        # Current position of the object\n",
    "        self.cur_pos = None\n",
    "\n",
    "    def can_overlap(self):\n",
    "        \"\"\"Can the agent overlap with this?\"\"\"\n",
    "        return False\n",
    "\n",
    "    def can_pickup(self):\n",
    "        \"\"\"Can the agent pick this up?\"\"\"\n",
    "        return False\n",
    "\n",
    "    def can_contain(self):\n",
    "        \"\"\"Can this contain another object?\"\"\"\n",
    "        return False\n",
    "\n",
    "    def see_behind(self):\n",
    "        \"\"\"Can the agent see behind this object?\"\"\"\n",
    "        return True\n",
    "\n",
    "    def toggle(self, env, pos):\n",
    "        \"\"\"Method to trigger/toggle an action this object performs\"\"\"\n",
    "        return False\n",
    "\n",
    "    def render(self, r):\n",
    "        \"\"\"Draw this object with the given renderer\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _set_color(self, r):\n",
    "        \"\"\"Set the color of this object as the active drawing color\"\"\"\n",
    "        c = COLORS[self.color]\n",
    "        r.setLineColor(c[0], c[1], c[2])\n",
    "        r.setColor(c[0], c[1], c[2])\n",
    "    def encode(self):\n",
    "        \"\"\"Encode the a description of this object as a 3-tuple of integers\"\"\"\n",
    "        return (OBJECT_TO_IDX[self.type], COLOR_TO_IDX[self.color], 0)\n",
    "\n",
    "\n",
    "class Goal(WorldObj):\n",
    "    def __init__(self):\n",
    "        super().__init__('goal', 'green')\n",
    "\n",
    "    def can_overlap(self):\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        self._set_color(r)\n",
    "        r.drawPolygon([\n",
    "            (0, CELL_PIXELS),\n",
    "            (CELL_PIXELS, CELL_PIXELS),\n",
    "            (CELL_PIXELS, 0),\n",
    "            (0, 0)\n",
    "        ])\n",
    "\n",
    "\n",
    "class Floor(WorldObj):\n",
    "    \"\"\"\n",
    "    Colored floor tile the agent can walk over\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color='blue'):\n",
    "        super().__init__('floor', color)\n",
    "\n",
    "    def can_overlap(self):\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        # Give the floor a pale color\n",
    "        c = COLORS[self.color]\n",
    "        r.setLineColor(100, 100, 100, 0)\n",
    "        r.setColor(*c / 2)\n",
    "        r.drawPolygon([\n",
    "            (1, CELL_PIXELS),\n",
    "            (CELL_PIXELS, CELL_PIXELS),\n",
    "            (CELL_PIXELS, 1),\n",
    "            (1, 1)\n",
    "        ])\n",
    "\n",
    "\n",
    "class Wall(WorldObj):\n",
    "    def __init__(self, color='grey'):\n",
    "        super().__init__('wall', color)\n",
    "\n",
    "    def see_behind(self):\n",
    "        return False\n",
    "\n",
    "    def render(self, r):\n",
    "        self._set_color(r)\n",
    "        r.drawPolygon([\n",
    "            (0, CELL_PIXELS),\n",
    "            (CELL_PIXELS, CELL_PIXELS),\n",
    "            (CELL_PIXELS, 0),\n",
    "            (0, 0)\n",
    "        ])\n",
    "\n",
    "\n",
    "class Door(WorldObj):\n",
    "    def __init__(self, color, is_open=False):\n",
    "        super().__init__('door', color)\n",
    "        self.is_open = is_open\n",
    "\n",
    "    def can_overlap(self):\n",
    "        \"\"\"The agent can only walk over this cell when the door is open\"\"\"\n",
    "        return self.is_open\n",
    "\n",
    "    def see_behind(self):\n",
    "        return self.is_open\n",
    "\n",
    "    def toggle(self, env, pos):\n",
    "        self.is_open = not self.is_open\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        c = COLORS[self.color]\n",
    "        r.setLineColor(c[0], c[1], c[2])\n",
    "        r.setColor(0, 0, 0)\n",
    "\n",
    "        if self.is_open:\n",
    "            r.drawPolygon([\n",
    "                (CELL_PIXELS - 2, CELL_PIXELS),\n",
    "                (CELL_PIXELS, CELL_PIXELS),\n",
    "                (CELL_PIXELS, 0),\n",
    "                (CELL_PIXELS - 2, 0)\n",
    "            ])\n",
    "            return\n",
    "\n",
    "        r.drawPolygon([\n",
    "            (0, CELL_PIXELS),\n",
    "            (CELL_PIXELS, CELL_PIXELS),\n",
    "            (CELL_PIXELS, 0),\n",
    "            (0, 0)\n",
    "        ])\n",
    "        r.drawPolygon([\n",
    "            (2, CELL_PIXELS - 2),\n",
    "            (CELL_PIXELS - 2, CELL_PIXELS - 2),\n",
    "            (CELL_PIXELS - 2, 2),\n",
    "            (2, 2)\n",
    "        ])\n",
    "        r.drawCircle(CELL_PIXELS * 0.75, CELL_PIXELS * 0.5, 2)\n",
    "\n",
    "\n",
    "class LockedDoor(WorldObj):\n",
    "    def __init__(self, color, is_open=False):\n",
    "        super(LockedDoor, self).__init__('locked_door', color)\n",
    "        self.is_open = is_open\n",
    "\n",
    "    def toggle(self, env, pos):\n",
    "        # If the player has the right key to open the door\n",
    "        if isinstance(env.carrying, Key) and env.carrying.color == self.color:\n",
    "            self.is_open = True\n",
    "            # The key has been used, remove it from the agent\n",
    "            env.carrying = None\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def can_overlap(self):\n",
    "        \"\"\"The agent can only walk over this cell when the door is open\"\"\"\n",
    "        return self.is_open\n",
    "\n",
    "    def see_behind(self):\n",
    "        return self.is_open\n",
    "\n",
    "    def render(self, r):\n",
    "        c = COLORS[self.color]\n",
    "        r.setLineColor(c[0], c[1], c[2])\n",
    "        r.setColor(c[0], c[1], c[2], 50)\n",
    "\n",
    "        if self.is_open:\n",
    "            r.drawPolygon([\n",
    "                (CELL_PIXELS - 2, CELL_PIXELS),\n",
    "                (CELL_PIXELS, CELL_PIXELS),\n",
    "                (CELL_PIXELS, 0),\n",
    "                (CELL_PIXELS - 2, 0)\n",
    "            ])\n",
    "            return\n",
    "\n",
    "        r.drawPolygon([\n",
    "            (0, CELL_PIXELS),\n",
    "            (CELL_PIXELS, CELL_PIXELS),\n",
    "            (CELL_PIXELS, 0),\n",
    "            (0, 0)\n",
    "        ])\n",
    "        r.drawPolygon([\n",
    "            (2, CELL_PIXELS - 2),\n",
    "            (CELL_PIXELS - 2, CELL_PIXELS - 2),\n",
    "            (CELL_PIXELS - 2, 2),\n",
    "            (2, 2)\n",
    "        ])\n",
    "        r.drawLine(\n",
    "            CELL_PIXELS * 0.55,\n",
    "            CELL_PIXELS * 0.5,\n",
    "            CELL_PIXELS * 0.75,\n",
    "            CELL_PIXELS * 0.5\n",
    "        )\n",
    "\n",
    "\n",
    "class Key(WorldObj):\n",
    "    def __init__(self, color='blue'):\n",
    "        super(Key, self).__init__('key', color)\n",
    "\n",
    "    def can_pickup(self):\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        self._set_color(r)\n",
    "\n",
    "        # Vertical quad\n",
    "        r.drawPolygon([\n",
    "            (16, 10),\n",
    "            (20, 10),\n",
    "            (20, 28),\n",
    "            (16, 28)\n",
    "        ])\n",
    "\n",
    "        # Teeth\n",
    "        r.drawPolygon([\n",
    "            (12, 19),\n",
    "            (16, 19),\n",
    "            (16, 21),\n",
    "            (12, 21)\n",
    "        ])\n",
    "        r.drawPolygon([\n",
    "            (12, 26),\n",
    "            (16, 26),\n",
    "            (16, 28),\n",
    "            (12, 28)\n",
    "        ])\n",
    "\n",
    "        r.drawCircle(18, 9, 6)\n",
    "        r.setLineColor(0, 0, 0)\n",
    "        r.setColor(0, 0, 0)\n",
    "        r.drawCircle(18, 9, 2)\n",
    "\n",
    "\n",
    "class Ball(WorldObj):\n",
    "    def __init__(self, color='blue'):\n",
    "        super(Ball, self).__init__('ball', color)\n",
    "\n",
    "    def can_pickup(self):\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        self._set_color(r)\n",
    "        r.drawCircle(CELL_PIXELS * 0.5, CELL_PIXELS * 0.5, 10)\n",
    "\n",
    "\n",
    "class Box(WorldObj):\n",
    "    def __init__(self, color, contains=None):\n",
    "        super(Box, self).__init__('box', color)\n",
    "        self.contains = contains\n",
    "\n",
    "    def can_pickup(self):\n",
    "        return True\n",
    "\n",
    "    def render(self, r):\n",
    "        c = COLORS[self.color]\n",
    "        r.setLineColor(c[0], c[1], c[2])\n",
    "        r.setColor(0, 0, 0)\n",
    "        r.setLineWidth(2)\n",
    "\n",
    "        r.drawPolygon([\n",
    "            (4, CELL_PIXELS - 4),\n",
    "            (CELL_PIXELS - 4, CELL_PIXELS - 4),\n",
    "            (CELL_PIXELS - 4, 4),\n",
    "            (4, 4)\n",
    "        ])\n",
    "\n",
    "        r.drawLine(\n",
    "            4,\n",
    "            CELL_PIXELS / 2,\n",
    "            CELL_PIXELS - 4,\n",
    "            CELL_PIXELS / 2\n",
    "        )\n",
    "\n",
    "        r.setLineWidth(1)\n",
    "\n",
    "    def toggle(self, env, pos):\n",
    "        # Replace the box by its contents\n",
    "        env.grid.set(*pos, self.contains)\n",
    "        return True\n",
    "\n",
    "\n",
    "class Grid:\n",
    "    \"\"\"\n",
    "    Represent a grid and operations on it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width, height):\n",
    "        assert width >= 4\n",
    "        assert height >= 4\n",
    "\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "\n",
    "        self.grid = [None] * width * height\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        if isinstance(key, WorldObj):\n",
    "            for e in self.grid:\n",
    "                if e is key:\n",
    "                    return True\n",
    "        elif isinstance(key, tuple):\n",
    "            for e in self.grid:\n",
    "                if e is None:\n",
    "                    continue\n",
    "                if (e.color, e.type) == key:\n",
    "                    return True\n",
    "                if key[0] is None and key[1] == e.type:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        grid1 = self.encode()\n",
    "        grid2 = other.encode()\n",
    "        return np.array_equal(grid2, grid1)\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self == other\n",
    "\n",
    "    def copy(self):\n",
    "        from copy import deepcopy\n",
    "        return deepcopy(self)\n",
    "\n",
    "    def set(self, i, j, v):\n",
    "        assert i >= 0 and i < self.width\n",
    "        assert j >= 0 and j < self.height\n",
    "        self.grid[j * self.width + i] = v\n",
    "\n",
    "    def get(self, i, j):\n",
    "        assert i >= 0 and i < self.width\n",
    "        assert j >= 0 and j < self.height\n",
    "        return self.grid[j * self.width + i]\n",
    "\n",
    "    def horz_wall(self, x, y, length=None):\n",
    "        if length is None:\n",
    "            length = self.width - x\n",
    "        for i in range(0, length):\n",
    "            self.set(x + i, y, Wall())\n",
    "\n",
    "    def vert_wall(self, x, y, length=None):\n",
    "        if length is None:\n",
    "            length = self.height - y\n",
    "        for j in range(0, length):\n",
    "            self.set(x, y + j, Wall())\n",
    "\n",
    "    def wall_rect(self, x, y, w, h):\n",
    "        self.horz_wall(x, y, w)\n",
    "        self.horz_wall(x, y + h - 1, w)\n",
    "        self.vert_wall(x, y, h)\n",
    "        self.vert_wall(x + w - 1, y, h)\n",
    "\n",
    "    def rotate_left(self):\n",
    "        \"\"\"\n",
    "        Rotate the grid to the left (counter-clockwise)\n",
    "        \"\"\"\n",
    "\n",
    "        grid = Grid(self.width, self.height)\n",
    "\n",
    "        for j in range(0, self.height):\n",
    "            for i in range(0, self.width):\n",
    "                v = self.get(self.width - 1 - j, i)\n",
    "                grid.set(i, j, v)\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def slice(self, topX, topY, width, height):\n",
    "        \"\"\"\n",
    "        Get a subset of the grid\n",
    "        \"\"\"\n",
    "\n",
    "        grid = Grid(width, height)\n",
    "\n",
    "        for j in range(0, height):\n",
    "            for i in range(0, width):\n",
    "                x = topX + i\n",
    "                y = topY + j\n",
    "\n",
    "                if x >= 0 and x < self.width and \\\n",
    "                        y >= 0 and y < self.height:\n",
    "                    v = self.get(x, y)\n",
    "                else:\n",
    "                    v = Wall()\n",
    "\n",
    "                grid.set(i, j, v)\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def render(self, r, tile_size):\n",
    "        \"\"\"\n",
    "        Render this grid at a given scale\n",
    "        :param r: target renderer object\n",
    "        :param tile_size: tile size in pixels\n",
    "        \"\"\"\n",
    "\n",
    "        assert r.width == self.width * tile_size\n",
    "        assert r.height == self.height * tile_size\n",
    "\n",
    "        # Total grid size at native scale\n",
    "        widthPx = self.width * CELL_PIXELS\n",
    "        heightPx = self.height * CELL_PIXELS\n",
    "\n",
    "        r.push()\n",
    "\n",
    "        # Internally, we draw at the \"large\" full-grid resolution, but we\n",
    "        # use the renderer to scale back to the desired size\n",
    "        r.scale(tile_size / CELL_PIXELS, tile_size / CELL_PIXELS)\n",
    "\n",
    "        # Draw the background of the in-world cells black\n",
    "        r.fillRect(\n",
    "            0,\n",
    "            0,\n",
    "            widthPx,\n",
    "            heightPx,\n",
    "            0, 0, 0\n",
    "        )\n",
    "\n",
    "        # Draw grid lines\n",
    "        r.setLineColor(100, 100, 100)\n",
    "        for rowIdx in range(0, self.height):\n",
    "            y = CELL_PIXELS * rowIdx\n",
    "            r.drawLine(0, y, widthPx, y)\n",
    "        for colIdx in range(0, self.width):\n",
    "            x = CELL_PIXELS * colIdx\n",
    "            r.drawLine(x, 0, x, heightPx)\n",
    "\n",
    "        # Render the grid\n",
    "        for j in range(0, self.height):\n",
    "            for i in range(0, self.width):\n",
    "                cell = self.get(i, j)\n",
    "                if cell == None:\n",
    "                    continue\n",
    "                r.push()\n",
    "                r.translate(i * CELL_PIXELS, j * CELL_PIXELS)\n",
    "                cell.render(r)\n",
    "                r.pop()\n",
    "\n",
    "        r.pop()\n",
    "\n",
    "    def encode(self):\n",
    "        \"\"\"\n",
    "        Produce a compact numpy encoding of the grid\n",
    "        \"\"\"\n",
    "\n",
    "        codeSize = self.width * self.height * 3\n",
    "\n",
    "        array = np.zeros(shape=(self.width, self.height, 3), dtype='uint8')\n",
    "\n",
    "        for j in range(0, self.height):\n",
    "            for i in range(0, self.width):\n",
    "\n",
    "                v = self.get(i, j)\n",
    "\n",
    "                if v == None:\n",
    "                    continue\n",
    "\n",
    "                array[i, j, 0] = OBJECT_TO_IDX[v.type]\n",
    "                array[i, j, 1] = COLOR_TO_IDX[v.color]\n",
    "\n",
    "                if hasattr(v, 'is_open') and v.is_open:\n",
    "                    array[i, j, 2] = 1\n",
    "\n",
    "        return array\n",
    "\n",
    "    def decode(array):\n",
    "        \"\"\"\n",
    "        Decode an array grid encoding back into a grid\n",
    "        \"\"\"\n",
    "\n",
    "        width = array.shape[0]\n",
    "        height = array.shape[1]\n",
    "        assert array.shape[2] == 3\n",
    "\n",
    "        grid = Grid(width, height)\n",
    "\n",
    "        for j in range(0, height):\n",
    "            for i in range(0, width):\n",
    "\n",
    "                typeIdx = array[i, j, 0]\n",
    "                colorIdx = array[i, j, 1]\n",
    "                openIdx = array[i, j, 2]\n",
    "\n",
    "                if typeIdx == 0:\n",
    "                    continue\n",
    "\n",
    "                objType = IDX_TO_OBJECT[typeIdx]\n",
    "                color = IDX_TO_COLOR[colorIdx]\n",
    "                is_open = True if openIdx == 1 else 0\n",
    "\n",
    "                if objType == 'wall':\n",
    "                    v = Wall(color)\n",
    "                elif objType == 'floor':\n",
    "                    v = Floor(color)\n",
    "                elif objType == 'ball':\n",
    "                    v = Ball(color)\n",
    "                elif objType == 'key':\n",
    "                    v = Key(color)\n",
    "                elif objType == 'box':\n",
    "                    v = Box(color)\n",
    "                elif objType == 'door':\n",
    "                    v = Door(color, is_open)\n",
    "                elif objType == 'locked_door':\n",
    "                    v = LockedDoor(color, is_open)\n",
    "                elif objType == 'goal':\n",
    "                    v = Goal()\n",
    "                else:\n",
    "                    assert False, \"unknown obj type in decode '%s'\" % objType\n",
    "\n",
    "                grid.set(i, j, v)\n",
    "\n",
    "        return grid\n",
    "\n",
    "    def process_vis(grid, agent_pos):\n",
    "        mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n",
    "\n",
    "        mask[agent_pos[0], agent_pos[1]] = True\n",
    "\n",
    "        for j in reversed(range(1, grid.height)):\n",
    "            for i in range(0, grid.width - 1):\n",
    "                if not mask[i, j]:\n",
    "                    continue\n",
    "\n",
    "                cell = grid.get(i, j)\n",
    "                if cell and not cell.see_behind():\n",
    "                    continue\n",
    "\n",
    "                mask[i + 1, j] = True\n",
    "                mask[i + 1, j - 1] = True\n",
    "                mask[i, j - 1] = True\n",
    "\n",
    "            for i in reversed(range(1, grid.width)):\n",
    "                if not mask[i, j]:\n",
    "                    continue\n",
    "\n",
    "                cell = grid.get(i, j)\n",
    "                if cell and not cell.see_behind():\n",
    "                    continue\n",
    "\n",
    "                mask[i - 1, j - 1] = True\n",
    "                mask[i - 1, j] = True\n",
    "                mask[i, j - 1] = True\n",
    "\n",
    "        for j in range(0, grid.height):\n",
    "            for i in range(0, grid.width):\n",
    "                if not mask[i, j]:\n",
    "                    grid.set(i, j, None)\n",
    "\n",
    "        return mask\n",
    "\n",
    "\n",
    "class MiniGridEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    2D grid world game environment\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array', 'pixmap'],\n",
    "        'video.frames_per_second': 10\n",
    "    }\n",
    "\n",
    "    # Enumeration of possible actions\n",
    "    class Actions(IntEnum):\n",
    "        # Turn left, turn right, move forward\n",
    "        left = 0\n",
    "        right = 1\n",
    "        forward = 2\n",
    "\n",
    "        # Pick up an object\n",
    "        pickup = 3\n",
    "        # Drop an object\n",
    "        drop = 4\n",
    "        # Toggle/activate an object\n",
    "        toggle = 5\n",
    "\n",
    "        # Done completing task\n",
    "        done = 6\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            grid_size=16,\n",
    "            max_steps=100,\n",
    "            see_through_walls=False,\n",
    "            seed=1337\n",
    "    ):\n",
    "        # Action enumeration for this environment\n",
    "        self.actions = MiniGridEnv.Actions\n",
    "\n",
    "        # Actions are discrete integer values\n",
    "        self.action_space = spaces.Discrete(len(self.actions))\n",
    "\n",
    "        # Observations are dictionaries containing an\n",
    "        # encoding of the grid and a textual 'mission' string\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=OBS_ARRAY_SIZE,\n",
    "            dtype='uint8'\n",
    "        )\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': self.observation_space\n",
    "        })\n",
    "\n",
    "        # Range of possible rewards\n",
    "        self.reward_range = (0, 1)\n",
    "\n",
    "        # Renderer object used to render the whole grid (full-scale)\n",
    "        self.grid_render = None\n",
    "\n",
    "        # Renderer used to render observations (small-scale agent view)\n",
    "        self.obs_render = None\n",
    "\n",
    "        # Environment configuration\n",
    "        self.grid_size = grid_size\n",
    "        self.max_steps = max_steps\n",
    "        self.see_through_walls = see_through_walls\n",
    "\n",
    "        # Starting position and direction for the agent\n",
    "        self.start_pos = None\n",
    "        self.start_dir = None\n",
    "\n",
    "        # Initialize the RNG\n",
    "        self.seed(seed=seed)\n",
    "        self.set_reward_multiplier(0.9)\n",
    "\n",
    "        # Initialize the state\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Generate a new random grid at the start of each episode\n",
    "        # To keep the same grid for each episode, call env.seed() with\n",
    "        # the same seed before calling env.reset()\n",
    "        self._gen_grid(self.grid_size, self.grid_size)\n",
    "\n",
    "        # These fields should be defined by _gen_grid\n",
    "        assert self.start_pos is not None\n",
    "        assert self.start_dir is not None\n",
    "\n",
    "        # Check that the agent doesn't overlap with an object\n",
    "        start_cell = self.grid.get(*self.start_pos)\n",
    "        assert start_cell is None or start_cell.can_overlap()\n",
    "\n",
    "        # Place the agent in the starting position and direction\n",
    "        self.agent_pos = self.start_pos\n",
    "        self.agent_dir = self.start_dir\n",
    "\n",
    "        # Item picked up, being carried, initially nothing\n",
    "        self.carrying = None\n",
    "\n",
    "        # Step count since episode start\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Return first observation\n",
    "        obs = self.gen_obs()\n",
    "        return obs\n",
    "\n",
    "    def set_wall_id(self, fixed_wall_id):\n",
    "        self.fixed_wall_id = fixed_wall_id\n",
    "\n",
    "    def seed(self, seed=1337):\n",
    "        # Seed the random number generator\n",
    "        self.np_random, _ = seeding.np_random(seed)\n",
    "        # Set not to use fixed wall\n",
    "        self.fixed_wall_id = -1\n",
    "        return [seed]\n",
    "\n",
    "    def set_reward_multiplier(self, reward_multiplier):\n",
    "        assert reward_multiplier > 0 and reward_multiplier <= 1\n",
    "        self.reward_multiplier = reward_multiplier\n",
    "\n",
    "    @property\n",
    "    def steps_remaining(self):\n",
    "        return self.max_steps - self.step_count\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Produce a pretty string of the environment's grid along with the agent.\n",
    "        The agent is represented by `⏩`. A grid pixel is represented by 2-character\n",
    "        string, the first one for the object and the second one for the color.\n",
    "        \"\"\"\n",
    "\n",
    "        from copy import deepcopy\n",
    "\n",
    "        def rotate_left(array):\n",
    "            new_array = deepcopy(array)\n",
    "            for i in range(len(array)):\n",
    "                for j in range(len(array[0])):\n",
    "                    new_array[j][len(array[0]) - 1 - i] = array[i][j]\n",
    "            return new_array\n",
    "\n",
    "        def vertically_symmetrize(array):\n",
    "            new_array = deepcopy(array)\n",
    "            for i in range(len(array)):\n",
    "                for j in range(len(array[0])):\n",
    "                    new_array[i][len(array[0]) - 1 - j] = array[i][j]\n",
    "            return new_array\n",
    "\n",
    "        # Map of object id to short string\n",
    "        OBJECT_IDX_TO_IDS = {\n",
    "            0: ' ',\n",
    "            1: 'W',\n",
    "            2: 'D',\n",
    "            3: 'L',\n",
    "            4: 'K',\n",
    "            5: 'B',\n",
    "            6: 'X',\n",
    "            7: 'G'\n",
    "        }\n",
    "\n",
    "        # Short string for opened door\n",
    "        OPENDED_DOOR_IDS = '_'\n",
    "\n",
    "        # Map of color id to short string\n",
    "        COLOR_IDX_TO_IDS = {\n",
    "            0: 'R',\n",
    "            1: 'G',\n",
    "            2: 'B',\n",
    "            3: 'P',\n",
    "            4: 'Y',\n",
    "            5: 'E'\n",
    "        }\n",
    "\n",
    "        # Map agent's direction to short string\n",
    "        AGENT_DIR_TO_IDS = {\n",
    "            0: '⏩ ',\n",
    "            1: '⏬ ',\n",
    "            2: '⏪ ',\n",
    "            3: '⏫ '\n",
    "        }\n",
    "\n",
    "        array = self.grid.encode()\n",
    "\n",
    "        array = rotate_left(array)\n",
    "        array = vertically_symmetrize(array)\n",
    "\n",
    "        new_array = []\n",
    "\n",
    "        for line in array:\n",
    "            new_line = []\n",
    "\n",
    "            for pixel in line:\n",
    "                # If the door is opened\n",
    "                if pixel[0] in [2, 3] and pixel[2] == 1:\n",
    "                    object_ids = OPENDED_DOOR_IDS\n",
    "                else:\n",
    "                    object_ids = OBJECT_IDX_TO_IDS[pixel[0]]\n",
    "\n",
    "                # If no object\n",
    "                if pixel[0] == 0:\n",
    "                    color_ids = ' '\n",
    "                else:\n",
    "                    color_ids = COLOR_IDX_TO_IDS[pixel[1]]\n",
    "\n",
    "                new_line.append(object_ids + color_ids)\n",
    "\n",
    "            new_array.append(new_line)\n",
    "\n",
    "        # Add the agent\n",
    "        new_array[self.agent_pos[1]][self.agent_pos[0]] = AGENT_DIR_TO_IDS[self.agent_dir]\n",
    "\n",
    "        return \"\\n\".join([\" \".join(line) for line in new_array])\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        assert False, \"_gen_grid needs to be implemented by each environment\"\n",
    "\n",
    "    def _reward(self):\n",
    "        \"\"\"\n",
    "        Compute the reward to be given upon success\n",
    "        self.reward_multiplier default 0.9\n",
    "        \"\"\"\n",
    "\n",
    "        return 1 - self.reward_multiplier * (self.step_count / self.max_steps)\n",
    "\n",
    "    def _rand_int(self, low, high):\n",
    "        \"\"\"\n",
    "        Generate random integer in [low,high[\n",
    "        \"\"\"\n",
    "\n",
    "        return self.np_random.randint(low, high)\n",
    "\n",
    "    def _rand_float(self, low, high):\n",
    "        \"\"\"\n",
    "        Generate random float in [low,high[\n",
    "        \"\"\"\n",
    "\n",
    "        return self.np_random.uniform(low, high)\n",
    "\n",
    "    def _rand_bool(self):\n",
    "        \"\"\"\n",
    "        Generate random boolean value\n",
    "        \"\"\"\n",
    "\n",
    "        return (self.np_random.randint(0, 2) == 0)\n",
    "\n",
    "    def _rand_elem(self, iterable):\n",
    "        \"\"\"\n",
    "        Pick a random element in a list\n",
    "        \"\"\"\n",
    "\n",
    "        lst = list(iterable)\n",
    "        idx = self._rand_int(0, len(lst))\n",
    "        return lst[idx]\n",
    "\n",
    "    def _rand_subset(self, iterable, num_elems):\n",
    "        \"\"\"\n",
    "        Sample a random subset of distinct elements of a list\n",
    "        \"\"\"\n",
    "\n",
    "        lst = list(iterable)\n",
    "        assert num_elems <= len(lst)\n",
    "\n",
    "        out = []\n",
    "\n",
    "        while len(out) < num_elems:\n",
    "            elem = self._rand_elem(lst)\n",
    "            lst.remove(elem)\n",
    "            out.append(elem)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _rand_color(self):\n",
    "        \"\"\"\n",
    "        Generate a random color name (string)\n",
    "        \"\"\"\n",
    "\n",
    "        return self._rand_elem(COLOR_NAMES)\n",
    "\n",
    "    def _rand_pos(self, xLow, xHigh, yLow, yHigh):\n",
    "        \"\"\"\n",
    "        Generate a random (x,y) position tuple\n",
    "        \"\"\"\n",
    "\n",
    "        return (\n",
    "            self.np_random.randint(xLow, xHigh),\n",
    "            self.np_random.randint(yLow, yHigh)\n",
    "        )\n",
    "\n",
    "    def place_obj(self,\n",
    "                  obj,\n",
    "                  top=None,\n",
    "                  size=None,\n",
    "                  reject_fn=None,\n",
    "                  max_tries=math.inf\n",
    "                  ):\n",
    "        \"\"\"\n",
    "        Place an object at an empty position in the grid\n",
    "        :param top: top-left position of the rectangle where to place\n",
    "        :param size: size of the rectangle where to place\n",
    "        :param reject_fn: function to filter out potential positions\n",
    "        \"\"\"\n",
    "\n",
    "        if top is None:\n",
    "            top = (0, 0)\n",
    "\n",
    "        if size is None:\n",
    "            size = (self.grid.width, self.grid.height)\n",
    "\n",
    "        num_tries = 0\n",
    "\n",
    "        while True:\n",
    "            # This is to handle with rare cases where rejection sampling\n",
    "            # gets stuck in an infinite loop\n",
    "            if num_tries > max_tries:\n",
    "                raise RecursionError('rejection sampling failed in place_obj')\n",
    "\n",
    "            num_tries += 1\n",
    "\n",
    "            pos = np.array((\n",
    "                self._rand_int(top[0], top[0] + size[0]),\n",
    "                self._rand_int(top[1], top[1] + size[1])\n",
    "            ))\n",
    "\n",
    "            # Don't place the object on top of another object\n",
    "            if self.grid.get(*pos) != None:\n",
    "                continue\n",
    "\n",
    "            # Don't place the object where the agent is\n",
    "            if np.array_equal(pos, self.start_pos):\n",
    "                continue\n",
    "\n",
    "            # Check if there is a filtering criterion\n",
    "            if reject_fn and reject_fn(self, pos):\n",
    "                continue\n",
    "\n",
    "            break\n",
    "\n",
    "        self.grid.set(*pos, obj)\n",
    "\n",
    "        if obj is not None:\n",
    "            obj.init_pos = pos\n",
    "            obj.cur_pos = pos\n",
    "\n",
    "        return pos\n",
    "\n",
    "    def place_agent(\n",
    "            self,\n",
    "            top=None,\n",
    "            size=None,\n",
    "            rand_dir=True,\n",
    "            max_tries=math.inf\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Set the agent's starting point at an empty position in the grid\n",
    "        \"\"\"\n",
    "\n",
    "        self.start_pos = None\n",
    "        pos = self.place_obj(None, top, size, max_tries=max_tries)\n",
    "        self.start_pos = pos\n",
    "\n",
    "        if rand_dir:\n",
    "            self.start_dir = self._rand_int(0, 4)\n",
    "\n",
    "        return pos\n",
    "\n",
    "    @property\n",
    "    def dir_vec(self):\n",
    "        \"\"\"\n",
    "        Get the direction vector for the agent, pointing in the direction\n",
    "        of forward movement.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.agent_dir >= 0 and self.agent_dir < 4\n",
    "        return DIR_TO_VEC[self.agent_dir]\n",
    "\n",
    "    @property\n",
    "    def right_vec(self):\n",
    "        \"\"\"\n",
    "        Get the vector pointing to the right of the agent.\n",
    "        \"\"\"\n",
    "\n",
    "        dx, dy = self.dir_vec\n",
    "        return np.array((-dy, dx))\n",
    "\n",
    "    @property\n",
    "    def front_pos(self):\n",
    "        \"\"\"\n",
    "        Get the position of the cell that is right in front of the agent\n",
    "        \"\"\"\n",
    "\n",
    "        return self.agent_pos + self.dir_vec\n",
    "\n",
    "    def get_view_coords(self, i, j):\n",
    "        \"\"\"\n",
    "        Translate and rotate absolute grid coordinates (i, j) into the\n",
    "        agent's partially observable view (sub-grid). Note that the resulting\n",
    "        coordinates may be negative or outside of the agent's view size.\n",
    "        \"\"\"\n",
    "\n",
    "        ax, ay = self.agent_pos\n",
    "        dx, dy = self.dir_vec\n",
    "        rx, ry = self.right_vec\n",
    "\n",
    "        # Compute the absolute coordinates of the top-left view corner\n",
    "        sz = AGENT_VIEW_SIZE\n",
    "        hs = AGENT_VIEW_SIZE // 2\n",
    "        tx = ax + (dx * (sz - 1)) - (rx * hs)\n",
    "        ty = ay + (dy * (sz - 1)) - (ry * hs)\n",
    "\n",
    "        lx = i - tx\n",
    "        ly = j - ty\n",
    "\n",
    "        # Project the coordinates of the object relative to the top-left\n",
    "        # corner onto the agent's own coordinate system\n",
    "        vx = (rx * lx + ry * ly)\n",
    "        vy = -(dx * lx + dy * ly)\n",
    "\n",
    "        return vx, vy\n",
    "\n",
    "    def get_view_exts(self):\n",
    "        \"\"\"\n",
    "        Get the extents of the square set of tiles visible to the agent\n",
    "        Note: the bottom extent indices are not included in the set\n",
    "        \"\"\"\n",
    "\n",
    "        # Facing right\n",
    "        if self.agent_dir == 0:\n",
    "            topX = self.agent_pos[0]\n",
    "            topY = self.agent_pos[1] - AGENT_VIEW_SIZE // 2\n",
    "        # Facing down\n",
    "        elif self.agent_dir == 1:\n",
    "            topX = self.agent_pos[0] - AGENT_VIEW_SIZE // 2\n",
    "            topY = self.agent_pos[1]\n",
    "        # Facing left\n",
    "        elif self.agent_dir == 2:\n",
    "            topX = self.agent_pos[0] - AGENT_VIEW_SIZE + 1\n",
    "            topY = self.agent_pos[1] - AGENT_VIEW_SIZE // 2\n",
    "        # Facing up\n",
    "        elif self.agent_dir == 3:\n",
    "            topX = self.agent_pos[0] - AGENT_VIEW_SIZE // 2\n",
    "            topY = self.agent_pos[1] - AGENT_VIEW_SIZE + 1\n",
    "        else:\n",
    "            assert False, \"invalid agent direction\"\n",
    "\n",
    "        botX = topX + AGENT_VIEW_SIZE\n",
    "        botY = topY + AGENT_VIEW_SIZE\n",
    "\n",
    "        return (topX, topY, botX, botY)\n",
    "\n",
    "    def agent_sees(self, x, y):\n",
    "        \"\"\"\n",
    "        Check if a grid position is visible to the agent\n",
    "        \"\"\"\n",
    "\n",
    "        vx, vy = self.get_view_coords(x, y)\n",
    "\n",
    "        if vx < 0 or vy < 0 or vx >= AGENT_VIEW_SIZE or vy >= AGENT_VIEW_SIZE:\n",
    "            return False\n",
    "\n",
    "        obs = self.gen_obs()\n",
    "        obs_grid = Grid.decode(obs['image'])\n",
    "        obs_cell = obs_grid.get(vx, vy)\n",
    "        world_cell = self.grid.get(x, y)\n",
    "\n",
    "        return obs_cell is not None and obs_cell.type == world_cell.type\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Get the position in front of the agent\n",
    "        fwd_pos = self.front_pos\n",
    "\n",
    "        # Get the contents of the cell in front of the agent\n",
    "        fwd_cell = self.grid.get(*fwd_pos)\n",
    "\n",
    "        # Rotate left\n",
    "        if action == self.actions.left:\n",
    "            self.agent_dir -= 1\n",
    "            if self.agent_dir < 0:\n",
    "                self.agent_dir += 4\n",
    "\n",
    "        # Rotate right\n",
    "        elif action == self.actions.right:\n",
    "            self.agent_dir = (self.agent_dir + 1) % 4\n",
    "\n",
    "        # Move forward\n",
    "        elif action == self.actions.forward:\n",
    "            if fwd_cell == None or fwd_cell.can_overlap():\n",
    "                self.agent_pos = fwd_pos\n",
    "            if fwd_cell != None and fwd_cell.type == 'goal':\n",
    "                done = True\n",
    "                reward = self._reward()\n",
    "\n",
    "        # Pick up an object\n",
    "        elif action == self.actions.pickup:\n",
    "            if fwd_cell and fwd_cell.can_pickup():\n",
    "                if self.carrying is None:\n",
    "                    self.carrying = fwd_cell\n",
    "                    self.carrying.cur_pos = np.array([-1, -1])\n",
    "                    self.grid.set(*fwd_pos, None)\n",
    "\n",
    "        # Drop an object\n",
    "        elif action == self.actions.drop:\n",
    "            if not fwd_cell and self.carrying:\n",
    "                self.grid.set(*fwd_pos, self.carrying)\n",
    "                self.carrying.cur_pos = fwd_pos\n",
    "                self.carrying = None\n",
    "\n",
    "        # Toggle/activate an object\n",
    "        elif action == self.actions.toggle:\n",
    "            if fwd_cell:\n",
    "                fwd_cell.toggle(self, fwd_pos)\n",
    "\n",
    "        # Done action (not used by default)\n",
    "        elif action == self.actions.done:\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            assert False, \"unknown action\"\n",
    "\n",
    "        if self.step_count >= self.max_steps:\n",
    "            done = True\n",
    "\n",
    "        obs = self.gen_obs()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def gen_obs_grid(self):\n",
    "        \"\"\"\n",
    "        Generate the sub-grid observed by the agent.\n",
    "        This method also outputs a visibility mask telling us which grid\n",
    "        cells the agent can actually see.\n",
    "        \"\"\"\n",
    "\n",
    "        topX, topY, botX, botY = self.get_view_exts()\n",
    "\n",
    "        grid = self.grid.slice(topX, topY, AGENT_VIEW_SIZE, AGENT_VIEW_SIZE)\n",
    "\n",
    "        for i in range(self.agent_dir + 1):\n",
    "            grid = grid.rotate_left()\n",
    "\n",
    "        # Process occluders and visibility\n",
    "        # Note that this incurs some performance cost\n",
    "        if not self.see_through_walls:\n",
    "            vis_mask = grid.process_vis(agent_pos=(AGENT_VIEW_SIZE // 2, AGENT_VIEW_SIZE - 1))\n",
    "        else:\n",
    "            vis_mask = np.ones(shape=(grid.width, grid.height), dtype=np.bool)\n",
    "\n",
    "        # Make it so the agent sees what it's carrying\n",
    "        # We do this by placing the carried object at the agent's position\n",
    "        # in the agent's partially observable view\n",
    "        agent_pos = grid.width // 2, grid.height - 1\n",
    "        if self.carrying:\n",
    "            grid.set(*agent_pos, self.carrying)\n",
    "        else:\n",
    "            grid.set(*agent_pos, None)\n",
    "\n",
    "        return grid, vis_mask\n",
    "\n",
    "    def gen_obs(self):\n",
    "        \"\"\"\n",
    "        Generate the agent's view (partially observable, low-resolution encoding)\n",
    "        \"\"\"\n",
    "\n",
    "        grid, vis_mask = self.gen_obs_grid()\n",
    "\n",
    "        # Encode the partially observable view into a numpy array\n",
    "        image = grid.encode()\n",
    "\n",
    "        assert hasattr(self, 'mission'), \"environments must define a textual mission string\"\n",
    "\n",
    "        # Observations are dictionaries containing:\n",
    "        # - an image (partially observable view of the environment)\n",
    "        # - the agent's direction/orientation (acting as a compass)\n",
    "        # - a textual mission string (instructions for the agent)\n",
    "        obs = {\n",
    "            'image': image,\n",
    "            'direction': self.agent_dir,\n",
    "            'mission': self.mission\n",
    "        }\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def get_obs_render(self, obs, tile_pixels=CELL_PIXELS // 2):\n",
    "        \"\"\"\n",
    "        Render an agent observation for visualization\n",
    "        \"\"\"\n",
    "\n",
    "        if self.obs_render == None:\n",
    "            from gym_minigrid.rendering import Renderer\n",
    "            self.obs_render = Renderer(\n",
    "                AGENT_VIEW_SIZE * tile_pixels,\n",
    "                AGENT_VIEW_SIZE * tile_pixels\n",
    "            )\n",
    "\n",
    "        r = self.obs_render\n",
    "\n",
    "        r.beginFrame()\n",
    "\n",
    "        grid = Grid.decode(obs)\n",
    "\n",
    "        # Render the whole grid\n",
    "        grid.render(r, tile_pixels)\n",
    "\n",
    "        # Draw the agent\n",
    "        ratio = tile_pixels / CELL_PIXELS\n",
    "        r.push()\n",
    "        r.scale(ratio, ratio)\n",
    "        r.translate(\n",
    "            CELL_PIXELS * (0.5 + AGENT_VIEW_SIZE // 2),\n",
    "            CELL_PIXELS * (AGENT_VIEW_SIZE - 0.5)\n",
    "        )\n",
    "        r.rotate(3 * 90)\n",
    "        r.setLineColor(255, 0, 0)\n",
    "        r.setColor(255, 0, 0)\n",
    "        r.drawPolygon([\n",
    "            (-12, 10),\n",
    "            (12, 0),\n",
    "            (-12, -10)\n",
    "        ])\n",
    "        r.pop()\n",
    "\n",
    "        r.endFrame()\n",
    "\n",
    "        return r.getPixmap()\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        \"\"\"\n",
    "        Render the whole-grid human view\n",
    "        \"\"\"\n",
    "\n",
    "        if close:\n",
    "            if self.grid_render:\n",
    "                self.grid_render.close()\n",
    "            return\n",
    "\n",
    "        if self.grid_render is None:\n",
    "            from gym_minigrid.rendering import Renderer\n",
    "            self.grid_render = Renderer(\n",
    "                self.grid_size * CELL_PIXELS,\n",
    "                self.grid_size * CELL_PIXELS,\n",
    "                True if mode == 'human' else False\n",
    "            )\n",
    "\n",
    "        r = self.grid_render\n",
    "\n",
    "        r.beginFrame()\n",
    "\n",
    "        # Render the whole grid\n",
    "        self.grid.render(r, CELL_PIXELS)\n",
    "\n",
    "        # Draw the agent\n",
    "        r.push()\n",
    "        r.translate(\n",
    "            CELL_PIXELS * (self.agent_pos[0] + 0.5),\n",
    "            CELL_PIXELS * (self.agent_pos[1] + 0.5)\n",
    "        )\n",
    "        r.rotate(self.agent_dir * 90)\n",
    "        r.setLineColor(255, 0, 0)\n",
    "        r.setColor(255, 0, 0)\n",
    "        r.drawPolygon([\n",
    "            (-12, 10),\n",
    "            (12, 0),\n",
    "            (-12, -10)\n",
    "        ])\n",
    "        r.pop()\n",
    "\n",
    "        # Compute which cells are visible to the agent\n",
    "        _, vis_mask = self.gen_obs_grid()\n",
    "\n",
    "        # Compute the absolute coordinates of the bottom-left corner\n",
    "        # of the agent's view area\n",
    "        f_vec = self.dir_vec\n",
    "        r_vec = self.right_vec\n",
    "        top_left = self.agent_pos + f_vec * (AGENT_VIEW_SIZE - 1) - r_vec * (AGENT_VIEW_SIZE // 2)\n",
    "\n",
    "        # For each cell in the visibility mask\n",
    "        for vis_j in range(0, AGENT_VIEW_SIZE):\n",
    "            for vis_i in range(0, AGENT_VIEW_SIZE):\n",
    "                # If this cell is not visible, don't highlight it\n",
    "                if not vis_mask[vis_i, vis_j]:\n",
    "                    continue\n",
    "\n",
    "                # Compute the world coordinates of this cell\n",
    "                abs_i, abs_j = top_left - (f_vec * vis_j) + (r_vec * vis_i)\n",
    "\n",
    "                # Highlight the cell\n",
    "                r.fillRect(\n",
    "                    abs_i * CELL_PIXELS,\n",
    "                    abs_j * CELL_PIXELS,\n",
    "                    CELL_PIXELS,\n",
    "                    CELL_PIXELS,\n",
    "                    255, 255, 255, 75\n",
    "                )\n",
    "\n",
    "        r.endFrame()\n",
    "\n",
    "        if mode == 'rgb_array':\n",
    "            return r.getArray()\n",
    "        elif mode == 'pixmap':\n",
    "            return r.getPixmap()\n",
    "\n",
    "        return r\n",
    "\n",
    "from gym_minigrid.register import register\n",
    "\n",
    "class DoorKeyEnv(MiniGridEnv):\n",
    "    \"\"\"\n",
    "    Environment with a door and key, sparse reward\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size=8):\n",
    "        super().__init__(\n",
    "            grid_size=size,\n",
    "            max_steps=10*size*size\n",
    "        )\n",
    "\n",
    "    def _gen_grid(self, width, height):\n",
    "        # Create an empty grid\n",
    "        self.grid = Grid(width, height)\n",
    "\n",
    "        # Generate the surrounding walls\n",
    "        self.grid.wall_rect(0, 0, width, height)\n",
    "\n",
    "        # Place a goal in the bottom-right corner\n",
    "        self.grid.set(width - 2, height - 2, Goal())\n",
    "\n",
    "        # Create a vertical splitting wall\n",
    "        splitIdx = self._rand_int(2, width-2)\n",
    "        self.grid.vert_wall(splitIdx, 0)\n",
    "\n",
    "        # Place the agent at a random position and orientation\n",
    "        # on the left side of the splitting wall\n",
    "        self.start_pos = self.place_agent(size=(splitIdx, height))\n",
    "\n",
    "        # Place a door in the wall\n",
    "        doorIdx = self._rand_int(1, width-2)\n",
    "        self.grid.set(splitIdx, doorIdx, LockedDoor('yellow'))\n",
    "\n",
    "        # Place a yellow key on the left side\n",
    "        self.place_obj(\n",
    "            obj=Key('yellow'),\n",
    "            top=(0, 0),\n",
    "            size=(splitIdx, height)\n",
    "        )\n",
    "\n",
    "        self.mission = \"use the key to open the door and then get to the goal\"\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=5)\n",
    "\n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=6)\n",
    "\n",
    "class DoorKeyEnv10x10(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=10)\n",
    "\n",
    "class DoorKeyEnv12x12(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=12)\n",
    "\n",
    "class DoorKeyEnv14x14(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=14)\n",
    "\n",
    "class DoorKeyEnv16x16(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=16)\n",
    "\n",
    "\n",
    "class DoorKeyEnv5x5(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=5)\n",
    "\n",
    "\n",
    "class DoorKeyEnv6x6(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=6)\n",
    "\n",
    "\n",
    "class DoorKeyEnv10x10(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=10)\n",
    "\n",
    "\n",
    "class DoorKeyEnv12x12(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=12)\n",
    "\n",
    "\n",
    "class DoorKeyEnv14x14(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=14)\n",
    "\n",
    "\n",
    "class DoorKeyEnv16x16(DoorKeyEnv):\n",
    "    def __init__(self):\n",
    "        super().__init__(size=16)\n",
    "def preprocess_images(images, device=None):\n",
    "    # Bug of Pytorch: very slow if not first converted to numpy array\n",
    "    images = np.array(images)\n",
    "    return torch.tensor(images, device=device, dtype=torch.float)\n",
    "\n",
    "def get_obss_preprocessor():\n",
    "    obs_space = {\"image\": [7,7,3]}\n",
    "\n",
    "    def preprocess_obss(obss, device=None):\n",
    "        return torch_ac.DictList({\n",
    "            \"image\": preprocess_images([obs[\"image\"] for obs in obss], device=device)\n",
    "        })\n",
    "\n",
    "    return obs_space, preprocess_obss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torch_ac\n",
    "import tensorboardX\n",
    "import sys\n",
    "sys.path.insert(0,'/root/server/Semantic-Loss/complex_constraints/curriculum learning/automatic-curriculum/')\n",
    "import utils\n",
    "import auto_curri as ac\n",
    "from model import ACModel\n",
    "import numpy as np\n",
    "class parser:\n",
    "    def __init__(self):\n",
    "        self.env=None#'MiniGrid-Unlock-v0'#'MiniGrid-BlockedUnlockPickup-v0'\n",
    "        self.curriculum='empty'\n",
    "        self.model=None\n",
    "        self.seed=1\n",
    "        self.log_interval=1\n",
    "        self.save_interval=10\n",
    "        self.procs=1\n",
    "        self.frames=10**7\n",
    "        self.epochs=4\n",
    "        self.batch_size=256\n",
    "        self.frames_per_proc=128\n",
    "        self.discount=0.99\n",
    "        self.lr=0.001\n",
    "        self.gae_lambda=0.95\n",
    "        self.entropy_coef=0.01\n",
    "        self.value_loss_coef=0.5\n",
    "        self.max_grad_norm=0.5\n",
    "        self.adam_eps=1e-8\n",
    "        self.clip_eps=0.2\n",
    "        self.lpe=\"Linreg\"\n",
    "        self.lpe_alpha=0.1\n",
    "        self.lpe_K=10\n",
    "        self.acp=\"MR\"\n",
    "        self.acp_MR_K=10\n",
    "        self.acp_MR_power=6\n",
    "        self.acp_MR_pot_prop=0.5\n",
    "        self.acp_MR_att_pred=0.2\n",
    "        self.acp_MR_att_succ=0.05\n",
    "        self.a2d=\"Prop\"\n",
    "        self.a2d_eps=0.1\n",
    "        self.a2d_tau=4e-4\n",
    "args = parser()\n",
    "args2 = parser()\n",
    "args2.curriculum='fetch'\n",
    "args3 = parser()\n",
    "args3.curriculum='doorkey'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "import torch_ac\n",
    "\n",
    "\n",
    "# Function from https://github.com/ikostrikov/pytorch-a2c-ppo-acktr/blob/master/model.py\n",
    "def init_params(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Linear\") != -1:\n",
    "        m.weight.data.normal_(0, 1)\n",
    "        m.weight.data *= 1 / torch.sqrt(m.weight.data.pow(2).sum(1, keepdim=True))\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "class ACModel(nn.Module, torch_ac.ACModel):\n",
    "    def __init__(self, obs_space, action_space):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define image embedding\n",
    "        self.image_embedding_size = 64\n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Define actor's model\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_space.n)\n",
    "        )\n",
    "\n",
    "        # Define critic's model\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "        # Initialize parameters correctly\n",
    "        self.apply(init_params)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = obs.image.transpose(1, 3).transpose(2, 3).to(device)\n",
    "        x = self.image_conv(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        \n",
    "        embedding = x\n",
    "        x = self.actor(embedding)\n",
    "        dist = Categorical(logits=F.log_softmax(x, dim=1))\n",
    "        \n",
    "        x = self.critic(embedding)\n",
    "        value = x.squeeze(1)\n",
    "\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_minigrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "device='cuda'\n",
    "# 输入所有agent的obs，输出所有agent的动作概率分布\n",
    "class G2ANet(nn.Module):\n",
    "    def __init__(self, training_state_size, args):\n",
    "        super(G2ANet, self).__init__()\n",
    "        self.image_embedding_size = 64\n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.training_state_size=training_state_size\n",
    "        # Encoding\n",
    "        self.encoding = nn.Linear(16+self.training_state_size, args.rnn_hidden_dim)  # 对所有agent的obs解码\n",
    "        self.h = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)  # 每个agent根据自己的obs编码得到hidden_state，用于记忆之前的obs\n",
    "\n",
    "        # Hard\n",
    "        # GRU输入[[h_i,h_1],[h_i,h_2],...[h_i,h_n]]与[0,...,0]，输出[[h_1],[h_2],...,[h_n]]与[h_n]， h_j表示了agent j与agent i的关系\n",
    "        # 输入的iputs维度为(n_agents - 1, batch_size * n_agents, rnn_hidden_dim * 2)，\n",
    "        # 即对于batch_size条数据，输入每个agent与其他n_agents - 1个agents的hidden_state的连接\n",
    "        self.hard_bi_GRU = nn.GRU(args.rnn_hidden_dim * 2, args.rnn_hidden_dim, bidirectional=True)\n",
    "        # 对h_j进行分析，得到agent j对于agent i的权重，输出两维，经过gumble_softmax后取其中一维即可，如果是0则不考虑agent j，如果是1则考虑\n",
    "        self.hard_encoding = nn.Linear(args.rnn_hidden_dim * 2, 2)  # 乘2因为是双向GRU，hidden_state维度为2 * hidden_dim\n",
    "\n",
    "        # Soft\n",
    "        self.q = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.k = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(args.rnn_hidden_dim, args.attention_dim)\n",
    "\n",
    "        # Decoding 输入自己的h_i与x_i，输出自己动作的概率分布\n",
    "        self.decoding = nn.Linear(args.rnn_hidden_dim + args.attention_dim, args.n_actions)\n",
    "        self.args = args\n",
    "        self.soft_weights=None\n",
    "        self.n=0\n",
    "    def forward(self, obs,training_state, hidden_state):\n",
    "        self.n+=1\n",
    "        obs =[ preprocess_obss([i]).image.transpose(1, 3).transpose(2, 3).to(device) for i in obs]\n",
    "        tmp=torch.cat((torch.cat((self.image_conv(obs[0]).reshape(obs[0].shape[0], -1).to('cuda'),training_state[0].reshape(1,self.training_state_size).to('cuda')),1),torch.cat((self.image_conv(obs[1]).reshape(obs[1].shape[0], -1).to('cuda'),training_state[1].reshape(1,self.training_state_size).to('cuda')),1)),0)\n",
    "        if len(obs)>2:\n",
    "            for i in range(2,len(obs)):\n",
    "                tmp=torch.cat((tmp,torch.cat((self.image_conv(obs[i]).reshape(obs[i].shape[0], -1).to('cuda'),training_state[i].reshape(1,self.training_state_size)),1)),0)\n",
    "        obs=tmp\n",
    "        size = obs.shape[0]  # batch_size * n_agents\n",
    "        # 先对obs编码\n",
    "        obs_encoding = f.relu(self.encoding(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        # 经过自己的GRU得到h\n",
    "        h_out = self.h(obs_encoding, h_in)  # (batch_size * n_agents, args.rnn_hidden_dim)\n",
    "\n",
    "        # Hard Attention，GRU和GRUCell不同，输入的维度是(序列长度,batch_size, dim)\n",
    "        if self.args.hard:\n",
    "            # Hard Attention前的准备\n",
    "            h = h_out.reshape(-1, self.args.n_agents, self.args.rnn_hidden_dim)  # 把h转化出n_agents维度，(batch_size, n_agents, rnn_hidden_dim)\n",
    "            input_hard = []\n",
    "            for i in range(self.args.n_agents):\n",
    "                h_i = h[:, i]  # (batch_size, rnn_hidden_dim)\n",
    "                h_hard_i = []\n",
    "                for j in range(self.args.n_agents):  # 对于agent i，把自己的h_i与其他agent的h分别拼接\n",
    "                    if j != i:\n",
    "                        h_hard_i.append(torch.cat([h_i, h[:, j]], dim=-1))\n",
    "                # j 循环结束之后，h_hard_i是一个list里面装着n_agents - 1个维度为(batch_size, rnn_hidden_dim * 2)的tensor\n",
    "                h_hard_i = torch.stack(h_hard_i, dim=0)\n",
    "                input_hard.append(h_hard_i)\n",
    "            # i循环结束之后，input_hard是一个list里面装着n_agents个维度为(n_agents - 1, batch_size, rnn_hidden_dim * 2)的tensor\n",
    "            input_hard = torch.stack(input_hard, dim=-2)\n",
    "            # 最终得到维度(n_agents - 1, batch_size * n_agents, rnn_hidden_dim * 2)，可以输入了\n",
    "            input_hard = input_hard.view(self.args.n_agents - 1, -1, self.args.rnn_hidden_dim * 2)\n",
    "\n",
    "            h_hard = torch.zeros((2 * 1, size, self.args.rnn_hidden_dim))  # 因为是双向GRU，每个GRU只有一层，所以第一维是2 * 1\n",
    "            if self.args.cuda:\n",
    "                h_hard = h_hard.to('cuda')\n",
    "            h_hard, _ = self.hard_bi_GRU(input_hard.to('cuda'), h_hard.to('cuda'))  # (n_agents - 1,batch_size * n_agents,rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.permute(1, 0, 2)  # (batch_size * n_agents, n_agents - 1, rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.reshape(-1, self.args.rnn_hidden_dim * 2)  # (batch_size * n_agents * (n_agents - 1), rnn_hidden_dim * 2)\n",
    "\n",
    "            # 得到hard权重, (n_agents, batch_size, 1,  n_agents - 1)，多出一个维度，下面加权求和的时候要用\n",
    "            hard_weights = self.hard_encoding(h_hard)\n",
    "            hard_weights = f.gumbel_softmax(hard_weights, tau=0.01)\n",
    "            # print(hard_weights)\n",
    "            hard_weights = hard_weights[:, 1].view(-1, self.args.n_agents, 1, self.args.n_agents - 1)\n",
    "            hard_weights = hard_weights.permute(1, 0, 2, 3)\n",
    "\n",
    "        else:\n",
    "            hard_weights = torch.ones((self.args.n_agents, size // self.args.n_agents, 1, self.args.n_agents - 1))\n",
    "            if self.args.cuda:\n",
    "                hard_weights = hard_weights.to('cuda')\n",
    "\n",
    "        # Soft Attention\n",
    "        q = self.q(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        k = self.k(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        v = f.relu(self.v(h_out)).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        x = []\n",
    "        for i in range(self.args.n_agents):\n",
    "            q_i = q[:, i].view(-1, 1, self.args.attention_dim)  # agent i的q，(batch_size, 1, args.attention_dim)\n",
    "            k_i = [k[:, j] for j in range(self.args.n_agents) if j != i]  # 对于agent i来说，其他agent的k\n",
    "            v_i = [v[:, j] for j in range(self.args.n_agents) if j != i]  # 对于agent i来说，其他agent的v\n",
    "\n",
    "            k_i = torch.stack(k_i, dim=0)  # (n_agents - 1, batch_size, args.attention_dim)\n",
    "            k_i = k_i.permute(1, 2, 0)  # 交换三个维度，变成(batch_size, args.attention_dim， n_agents - 1)\n",
    "            v_i = torch.stack(v_i, dim=0)\n",
    "            v_i = v_i.permute(1, 2, 0)\n",
    "\n",
    "            # (batch_size, 1, attention_dim) * (batch_size, attention_dim，n_agents - 1) = (batch_size, 1，n_agents - 1)\n",
    "            score = torch.matmul(q_i, k_i)\n",
    "\n",
    "            # 归一化\n",
    "            scaled_score = score / np.sqrt(self.args.attention_dim)\n",
    "\n",
    "            # softmax得到权重\n",
    "            soft_weight = f.softmax(scaled_score, dim=-1)  # (batch_size，1, n_agents - 1)\n",
    "            if i==0:\n",
    "                soft_weights=soft_weight\n",
    "            else:\n",
    "                soft_weights=torch.cat((soft_weights,soft_weight),dim=0)\n",
    "            # 加权求和，注意三个矩阵的最后一维是n_agents - 1维度，得到(batch_size, args.attention_dim)\n",
    "            x_i = (v_i * soft_weight * hard_weights[i]).sum(dim=-1)\n",
    "            x.append(x_i)\n",
    "        if self.soft_weights is None:\n",
    "            self.soft_weights=soft_weights\n",
    "        else:\n",
    "            self.soft_weights=(self.soft_weights*(self.n-1)+soft_weights)/self.n        \n",
    "      #  if self.n%1000==0:\n",
    "       #     print(self.soft_weights)\n",
    "        # 合并每个agent的h与x\n",
    "        x = torch.stack(x, dim=1).reshape(-1, self.args.attention_dim)  # (batch_size * n_agents, args.attention_dim)\n",
    "        final_input = torch.cat((h_out, x), dim=1)\n",
    "       # output = self.decoding(final_input)\n",
    "\n",
    "        return final_input\n",
    "    \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.n\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "import torch.optim as optim\n",
    "class g2apara:\n",
    "    def __init__(self):\n",
    "        self.rnn_hidden_dim=1\n",
    "        self.n_agents=13\n",
    "        self.attention_dim=15\n",
    "        self.cuda=True\n",
    "        self.n_actions=7\n",
    "        self.hard=True\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space,n_agents,n_hid):\n",
    "        self.arg4=g2apara()\n",
    "        self.arg4.n_agents=n_agents\n",
    "        self.g2a=G2ANet(n_hid,self.arg4)\n",
    "        self.g2a.to('cuda')\n",
    "        self.hids=torch.from_numpy(np.random.random([self.arg4.n_agents,self.arg4.rnn_hidden_dim])).float().to('cuda')\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.to('cuda')\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer2 = optim.Adam(self.g2a.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        new_states=self.g2a.forward(state,torch.tensor([[1.,1.] for _ in range(self.arg4.n_agents)]).to('cuda'),self.hids)\n",
    "        probs = self.model(new_states)\n",
    "        action = [probs[i].multinomial(1).data for i in range(self.arg4.n_agents)]\n",
    "        #print(probs)\n",
    "        prob = [probs[i][action[i][0]].view(1, -1) for i in range(self.arg4.n_agents)]\n",
    "        log_prob = [prob[i].log() for i in range(self.arg4.n_agents)]\n",
    "        entropy = [- (probs[i]*probs[i].log()).sum() for i in range(self.arg4.n_agents)]\n",
    "\n",
    "        return [action[i][0] for i in range(self.arg4.n_agents)], log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for j in range(arg4.n_agents):\n",
    "            tmp=0\n",
    "            for i in reversed(range(len(rewards[j]))):\n",
    "                R = gamma * R + rewards[j][i]\n",
    "                tmp = tmp - (log_probs[j][i]*(Variable(R).expand_as(log_probs[j][i])).to('cuda')).sum() - (0.0001*entropies[j][i].to('cuda')).sum()\n",
    "            loss +=10* tmp / len(rewards[j])\n",
    "       # print('loss:',loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs0=[[gym_minigrid.envs.DistShift1(),\n",
    "gym_minigrid.envs.DistShift2(),\n",
    "gym_minigrid.envs.DistShiftEnv()], \n",
    "[DoorKeyEnv5x5(),\n",
    " DoorKeyEnv6x6(),DoorKeyEnv(),DoorKeyEnv10x10(),DoorKeyEnv12x12(),DoorKeyEnv14x14(),DoorKeyEnv16x16()],\n",
    "[gym_minigrid.envs.DynamicObstaclesEnv5x5(),\n",
    "gym_minigrid.envs.DynamicObstaclesEnv6x6(),\n",
    "gym_minigrid.envs.DynamicObstaclesRandomEnv5x5(),\n",
    "gym_minigrid.envs.DynamicObstaclesRandomEnv6x6(),\n",
    "gym_minigrid.envs.DynamicObstaclesEnv(),\n",
    "gym_minigrid.envs.DynamicObstaclesEnv16x16(),],\n",
    "[gym_minigrid.envs.EmptyEnv5x5(),\n",
    "gym_minigrid.envs.EmptyEnv6x6(),\n",
    "gym_minigrid.envs.EmptyRandomEnv5x5(),\n",
    "gym_minigrid.envs.EmptyRandomEnv6x6(),\n",
    "gym_minigrid.envs.EmptyEnv(),\n",
    "gym_minigrid.envs.EmptyEnv16x16(),],\n",
    "[gym_minigrid.envs.FetchEnv5x5N2(),\n",
    "gym_minigrid.envs.FetchEnv6x6N2(),\n",
    "gym_minigrid.envs.FetchEnv()],\n",
    "#[gym_minigrid.envs.GoToDoor6x6Env(),\n",
    "#gym_minigrid.envs.GoToDoor8x8Env(),\n",
    "#gym_minigrid.envs.GoToDoorEnv(),\n",
    "#gym_minigrid.envs.GoToObjectEnv(),\n",
    "#gym_minigrid.envs.GotoEnv8x8N2()],\n",
    "[gym_minigrid.envs.KeyCorridorS3R1(),\n",
    "gym_minigrid.envs.KeyCorridorS3R2(),\n",
    "gym_minigrid.envs.KeyCorridorS3R3(),\n",
    "gym_minigrid.envs.KeyCorridorS4R3(),\n",
    "gym_minigrid.envs.KeyCorridorS5R3(),\n",
    "gym_minigrid.envs.KeyCorridorS6R3(),\n",
    "gym_minigrid.envs.KeyCorridor()],\n",
    "[#gym_minigrid.envs.Lava(),\n",
    "gym_minigrid.envs.LavaCrossingEnv(),\n",
    "gym_minigrid.envs.LavaCrossingS11N5Env(),\n",
    "gym_minigrid.envs.LavaCrossingS9N2Env(),\n",
    "gym_minigrid.envs.LavaCrossingS9N3Env()],\n",
    "[#gym_minigrid.envs.LavaGapEnv(),\n",
    "gym_minigrid.envs.LavaGapS5Env(),\n",
    "gym_minigrid.envs.LavaGapS6Env(),\n",
    "gym_minigrid.envs.LavaGapS7Env()],\n",
    "[#gym_minigrid.envs.MemoryEnv(),\n",
    "gym_minigrid.envs.MemoryS7(),\n",
    "gym_minigrid.envs.MemoryS9(),\n",
    "gym_minigrid.envs.MemoryS11(),\n",
    "gym_minigrid.envs.MemoryS13(),\n",
    "gym_minigrid.envs.MemoryS13Random(),\n",
    "gym_minigrid.envs.MemoryS17Random()],\n",
    "[gym_minigrid.envs.MultiRoomEnvN2S4(),\n",
    "gym_minigrid.envs.MultiRoomEnvN4S5(),\n",
    "gym_minigrid.envs.MultiRoomEnvN6(),\n",
    "gym_minigrid.envs.MultiRoomEnv(8,8)],\n",
    "[#gym_minigrid.envs.ObstructedMazeEnv(),\n",
    "gym_minigrid.envs.ObstructedMaze_1Dl(),\n",
    "gym_minigrid.envs.ObstructedMaze_1Dlh(),\n",
    "gym_minigrid.envs.ObstructedMaze_1Dlhb(),\n",
    "gym_minigrid.envs.ObstructedMaze_1Q(),\n",
    "gym_minigrid.envs.ObstructedMaze_2Dl(),\n",
    "gym_minigrid.envs.ObstructedMaze_2Dlh(),\n",
    "gym_minigrid.envs.ObstructedMaze_2Dlhb(),\n",
    "gym_minigrid.envs.ObstructedMaze_2Q(),\n",
    "gym_minigrid.envs.ObstructedMaze_Full()],\n",
    "[gym_minigrid.envs.SimpleCrossingEnv(),\n",
    "gym_minigrid.envs.SimpleCrossingS9N2Env(),\n",
    "gym_minigrid.envs.SimpleCrossingS9N3Env(),\n",
    "gym_minigrid.envs.SimpleCrossingS11N5Env()],\n",
    "[gym_minigrid.envs.unlock.Unlock(),\n",
    "gym_minigrid.envs.unlockpickup.UnlockPickup(),\n",
    "gym_minigrid.envs.BlockedUnlockPickup()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_episode 0\n",
      "4 0.9928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -1\n",
      "8 0.9448979591836735\n",
      "1 0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:218: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_episode 1\n",
      "2 -1\n",
      "4 0.8488\n",
      "12 0.93125\n",
      "3 0.757\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a15b80bc8d91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mrewardsRec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-03146e776024>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[0;34m(self, rewards, log_probs, entropies, gamma)\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse, math, os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        action = action * 2 - 1\n",
    "        return actions\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "arg4=g2apara()\n",
    "agent = REINFORCE(64,15+arg4.rnn_hidden_dim, gym.spaces.Discrete(7) ,arg4.n_agents,2)\n",
    "\n",
    "envs=[i[0] for i in envs0]\n",
    "obs_space, preprocess_obss=get_obss_preprocessor()\n",
    "#obs=envs[-1].reset()\n",
    "#obs['image']=np.zeros([7,7,3])\n",
    "#acmodel(preprocess_obss([obs]))[0].sample()\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    print('i_episode',i_episode)\n",
    "    visualise=True\n",
    "    rewardsRec=[[] for _ in range(arg4.n_agents)]\n",
    "    reward=[0 for i in range(arg4.n_agents)]\n",
    "    rewardcnt=[0 for i in range(arg4.n_agents)]\n",
    "    done=[0 for i in range(arg4.n_agents)]\n",
    "    observations =  [envs[i].reset() for i in range(arg4.n_agents)]\n",
    "    entropies = [[] for _ in range(arg4.n_agents)]\n",
    "    log_probs = [[] for _ in range(arg4.n_agents)]\n",
    "    rewards = [[] for _ in range(arg4.n_agents)]\n",
    "    flag=[0 for _ in range(arg4.n_agents)]\n",
    "    for t in range(256):\n",
    "        action, log_prob, entropy = agent.select_action(observations)\n",
    "     #   action = action.cpu()\n",
    "        for i in range(arg4.n_agents):\n",
    "            observations[i], reward[i], done[i], _ = envs[i].step(action[i])\n",
    "            if flag[i]==0:\n",
    "                entropies[i].append(entropy[i])\n",
    "                log_probs[i].append(log_prob[i])\n",
    "                rewards[i].append(reward[i])\n",
    "                if reward[i]:\n",
    "                    print(i,reward[i])\n",
    "            if done[i]:\n",
    "                flag[i]=1\n",
    "                observations[i] =  envs[i].reset()\n",
    "        if sum(done)==arg4.n_agents:      \n",
    "            for  i in range(arg4.n_agents):\n",
    "                rewardsRec[i].append(np.array(rewards[i]).mean())\n",
    "            break\n",
    "    agent.update_parameters(rewards, log_probs, entropies, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym_minigrid\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import math\n",
    "\n",
    "device='cuda'\n",
    "# 输入所有agent的obs，输出所有agent的动作概率分布\n",
    "class G2ANet(nn.Module):\n",
    "    def __init__(self, training_state_size, args):\n",
    "        super(G2ANet, self).__init__()\n",
    "        self.image_embedding_size = 64\n",
    "        self.image_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(kernel_size=(2, 2), stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(2, 2)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.training_state_size=training_state_size\n",
    "        # Encoding\n",
    "        self.encoding = nn.Linear(16+self.training_state_size, args.rnn_hidden_dim)  # 对所有agent的obs解码\n",
    "        self.h = nn.GRUCell(args.rnn_hidden_dim, args.rnn_hidden_dim)  # 每个agent根据自己的obs编码得到hidden_state，用于记忆之前的obs\n",
    "\n",
    "        # Hard\n",
    "        # GRU输入[[h_i,h_1],[h_i,h_2],...[h_i,h_n]]与[0,...,0]，输出[[h_1],[h_2],...,[h_n]]与[h_n]， h_j表示了agent j与agent i的关系\n",
    "        # 输入的iputs维度为(n_agents - 1, batch_size * n_agents, rnn_hidden_dim * 2)，\n",
    "        # 即对于batch_size条数据，输入每个agent与其他n_agents - 1个agents的hidden_state的连接\n",
    "        self.hard_bi_GRU = nn.GRU(args.rnn_hidden_dim * 2, args.rnn_hidden_dim, bidirectional=True)\n",
    "        # 对h_j进行分析，得到agent j对于agent i的权重，输出两维，经过gumble_softmax后取其中一维即可，如果是0则不考虑agent j，如果是1则考虑\n",
    "        self.hard_encoding = nn.Linear(args.rnn_hidden_dim * 2, 2)  # 乘2因为是双向GRU，hidden_state维度为2 * hidden_dim\n",
    "\n",
    "        # Soft\n",
    "        self.q = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.k = nn.Linear(args.rnn_hidden_dim, args.attention_dim, bias=False)\n",
    "        self.v = nn.Linear(args.rnn_hidden_dim, args.attention_dim)\n",
    "\n",
    "        # Decoding 输入自己的h_i与x_i，输出自己动作的概率分布\n",
    "        self.decoding = nn.Linear(args.rnn_hidden_dim + args.attention_dim, args.n_actions)\n",
    "        self.args = args\n",
    "        self.soft_weights=None\n",
    "        self.n=0\n",
    "    def forward(self, obs,training_state, hidden_state):\n",
    "        self.n+=1\n",
    "        obs =[ preprocess_obss([i]).image.transpose(1, 3).transpose(2, 3).to(device) for i in obs]\n",
    "        tmp=torch.cat((torch.cat((self.image_conv(obs[0]).reshape(obs[0].shape[0], -1).to('cuda'),training_state[0].reshape(1,self.training_state_size).to('cuda')),1),torch.cat((self.image_conv(obs[1]).reshape(obs[1].shape[0], -1).to('cuda'),training_state[1].reshape(1,self.training_state_size).to('cuda')),1)),0)\n",
    "        if len(obs)>2:\n",
    "            for i in range(2,len(obs)):\n",
    "                tmp=torch.cat((tmp,torch.cat((self.image_conv(obs[i]).reshape(obs[i].shape[0], -1).to('cuda'),training_state[i].reshape(1,self.training_state_size)),1)),0)\n",
    "        obs=tmp\n",
    "        size = obs.shape[0]  # batch_size * n_agents\n",
    "        # 先对obs编码\n",
    "        obs_encoding = f.relu(self.encoding(obs))\n",
    "        h_in = hidden_state.reshape(-1, self.args.rnn_hidden_dim)\n",
    "        # 经过自己的GRU得到h\n",
    "        h_out = self.h(obs_encoding, h_in)  # (batch_size * n_agents, args.rnn_hidden_dim)\n",
    "\n",
    "        # Hard Attention，GRU和GRUCell不同，输入的维度是(序列长度,batch_size, dim)\n",
    "        if self.args.hard:\n",
    "            # Hard Attention前的准备\n",
    "            h = h_out.reshape(-1, self.args.n_agents, self.args.rnn_hidden_dim)  # 把h转化出n_agents维度，(batch_size, n_agents, rnn_hidden_dim)\n",
    "            input_hard = []\n",
    "            for i in range(self.args.n_agents):\n",
    "                h_i = h[:, i]  # (batch_size, rnn_hidden_dim)\n",
    "                h_hard_i = []\n",
    "                for j in range(self.args.n_agents):  # 对于agent i，把自己的h_i与其他agent的h分别拼接\n",
    "                    if j != i:\n",
    "                        h_hard_i.append(torch.cat([h_i, h[:, j]], dim=-1))\n",
    "                # j 循环结束之后，h_hard_i是一个list里面装着n_agents - 1个维度为(batch_size, rnn_hidden_dim * 2)的tensor\n",
    "                h_hard_i = torch.stack(h_hard_i, dim=0)\n",
    "                input_hard.append(h_hard_i)\n",
    "            # i循环结束之后，input_hard是一个list里面装着n_agents个维度为(n_agents - 1, batch_size, rnn_hidden_dim * 2)的tensor\n",
    "            input_hard = torch.stack(input_hard, dim=-2)\n",
    "            # 最终得到维度(n_agents - 1, batch_size * n_agents, rnn_hidden_dim * 2)，可以输入了\n",
    "            input_hard = input_hard.view(self.args.n_agents - 1, -1, self.args.rnn_hidden_dim * 2)\n",
    "\n",
    "            h_hard = torch.zeros((2 * 1, size, self.args.rnn_hidden_dim))  # 因为是双向GRU，每个GRU只有一层，所以第一维是2 * 1\n",
    "            if self.args.cuda:\n",
    "                h_hard = h_hard.to('cuda')\n",
    "            h_hard, _ = self.hard_bi_GRU(input_hard.to('cuda'), h_hard.to('cuda'))  # (n_agents - 1,batch_size * n_agents,rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.permute(1, 0, 2)  # (batch_size * n_agents, n_agents - 1, rnn_hidden_dim * 2)\n",
    "            h_hard = h_hard.reshape(-1, self.args.rnn_hidden_dim * 2)  # (batch_size * n_agents * (n_agents - 1), rnn_hidden_dim * 2)\n",
    "\n",
    "            # 得到hard权重, (n_agents, batch_size, 1,  n_agents - 1)，多出一个维度，下面加权求和的时候要用\n",
    "            hard_weights = self.hard_encoding(h_hard)\n",
    "            hard_weights = f.gumbel_softmax(hard_weights, tau=0.01)\n",
    "            # print(hard_weights)\n",
    "            hard_weights = hard_weights[:, 1].view(-1, self.args.n_agents, 1, self.args.n_agents - 1)\n",
    "            hard_weights = hard_weights.permute(1, 0, 2, 3)\n",
    "\n",
    "        else:\n",
    "            hard_weights = torch.ones((self.args.n_agents, size // self.args.n_agents, 1, self.args.n_agents - 1))\n",
    "            if self.args.cuda:\n",
    "                hard_weights = hard_weights.to('cuda')\n",
    "\n",
    "        # Soft Attention\n",
    "        q = self.q(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        k = self.k(h_out).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        v = f.relu(self.v(h_out)).reshape(-1, self.args.n_agents, self.args.attention_dim)  # (batch_size, n_agents, args.attention_dim)\n",
    "        x = []\n",
    "        for i in range(self.args.n_agents):\n",
    "            q_i = q[:, i].view(-1, 1, self.args.attention_dim)  # agent i的q，(batch_size, 1, args.attention_dim)\n",
    "            k_i = [k[:, j] for j in range(self.args.n_agents) if j != i]  # 对于agent i来说，其他agent的k\n",
    "            v_i = [v[:, j] for j in range(self.args.n_agents) if j != i]  # 对于agent i来说，其他agent的v\n",
    "\n",
    "            k_i = torch.stack(k_i, dim=0)  # (n_agents - 1, batch_size, args.attention_dim)\n",
    "            k_i = k_i.permute(1, 2, 0)  # 交换三个维度，变成(batch_size, args.attention_dim， n_agents - 1)\n",
    "            v_i = torch.stack(v_i, dim=0)\n",
    "            v_i = v_i.permute(1, 2, 0)\n",
    "\n",
    "            # (batch_size, 1, attention_dim) * (batch_size, attention_dim，n_agents - 1) = (batch_size, 1，n_agents - 1)\n",
    "            score = torch.matmul(q_i, k_i)\n",
    "\n",
    "            # 归一化\n",
    "            scaled_score = score / np.sqrt(self.args.attention_dim)\n",
    "\n",
    "            # softmax得到权重\n",
    "            soft_weight = f.softmax(scaled_score, dim=-1)  # (batch_size，1, n_agents - 1)\n",
    "            if i==0:\n",
    "                soft_weights=soft_weight\n",
    "            else:\n",
    "                soft_weights=torch.cat((soft_weights,soft_weight),dim=0)\n",
    "            # 加权求和，注意三个矩阵的最后一维是n_agents - 1维度，得到(batch_size, args.attention_dim)\n",
    "            x_i = (v_i * soft_weight * hard_weights[i]).sum(dim=-1)\n",
    "            x.append(x_i)\n",
    "        if self.soft_weights is None:\n",
    "            self.soft_weights=soft_weights\n",
    "        else:\n",
    "            self.soft_weights=(self.soft_weights*(self.n-1)+soft_weights)/self.n        \n",
    "      #  if self.n%1000==0:\n",
    "       #     print(self.soft_weights)\n",
    "        # 合并每个agent的h与x\n",
    "        x = torch.stack(x, dim=1).reshape(-1, self.args.attention_dim)  # (batch_size * n_agents, args.attention_dim)\n",
    "        final_input = torch.cat((h_out, x), dim=1)\n",
    "       # output = self.decoding(final_input)\n",
    "\n",
    "        return final_input\n",
    "    \n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import torchvision.transforms as T\n",
    "from torch.autograd import Variable\n",
    "import pdb\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    def __init__(self, hidden_size, num_inputs, action_space):\n",
    "        super(Policy, self).__init__()\n",
    "        self.action_space = action_space\n",
    "        num_outputs = action_space.n\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, num_outputs)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs\n",
    "        x = F.relu(self.linear1(x))\n",
    "        action_scores = self.linear2(x)\n",
    "        return F.softmax(action_scores)\n",
    "\n",
    "import torch.optim as optim\n",
    "class g2apara:\n",
    "    def __init__(self):\n",
    "        self.rnn_hidden_dim=1\n",
    "        self.n_agents=13\n",
    "        self.attention_dim=15\n",
    "        self.cuda=True\n",
    "        self.n_actions=7\n",
    "        self.hard=False\n",
    "\n",
    "class REINFORCE:\n",
    "    def __init__(self, hidden_size, num_inputs, action_space,n_agents,n_hid):\n",
    "        self.arg4=g2apara()\n",
    "        self.arg4.n_agents=n_agents\n",
    "        self.g2a=G2ANet(n_hid,self.arg4)\n",
    "        self.g2a.to('cuda')\n",
    "        self.hids=torch.from_numpy(np.random.random([self.arg4.n_agents,self.arg4.rnn_hidden_dim])).float().to('cuda')\n",
    "        self.action_space = action_space\n",
    "        self.model = Policy(hidden_size, num_inputs, action_space)\n",
    "        self.model = self.model.to('cuda')\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.optimizer2 = optim.Adam(self.g2a.parameters(), lr=1e-3)\n",
    "        self.model.train()\n",
    "\n",
    "    def select_action(self, state,task):\n",
    "        probs = self.model(self.g2a.forward(state,torch.tensor([[1.,1.] for _ in range(self.arg4.n_agents)]).to('cuda'),self.hids)[task])\n",
    "        action = probs.multinomial(1).data\n",
    "        #print(probs)\n",
    "        prob = probs[action[0]].view(1, -1) \n",
    "        log_prob = prob.log()\n",
    "        entropy = - (probs*probs.log()).sum()\n",
    "\n",
    "        return action[0], log_prob, entropy\n",
    "\n",
    "    def update_parameters(self, rewards, log_probs, entropies, gamma):\n",
    "        R = torch.zeros(1, 1)\n",
    "        loss = 0\n",
    "        for j in range(arg4.n_agents):\n",
    "            tmp=0\n",
    "            for i in reversed(range(len(rewards[j]))):\n",
    "                R = gamma * R + rewards[j][i]\n",
    "                tmp = tmp - (log_probs[j][i]*(Variable(R).expand_as(log_probs[j][i])).to('cuda')).sum() - (0.0001*entropies[j][i].to('cuda')).sum()\n",
    "            loss +=10* tmp / len(rewards[j])\n",
    "       # print('loss:',loss)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss.backward()\n",
    "        utils.clip_grad_norm(self.model.parameters(), 40)\n",
    "        self.optimizer.step()\n",
    "        self.optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i_episode 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 251\n",
      "1\n",
      "over 249\n",
      "2\n",
      "2 -1\n",
      "over 51\n",
      "3\n",
      "3 0.604\n",
      "over 43\n",
      "4\n",
      "4 0.892\n",
      "over 14\n",
      "5\n",
      "over 269\n",
      "6\n",
      "over 2\n",
      "7\n",
      "over 13\n",
      "8\n",
      "over 244\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "import argparse, math, os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.utils as utils\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper):\n",
    "\n",
    "    def _action(self, action):\n",
    "        action = (action + 1) / 2  # [-1, 1] => [0, 1]\n",
    "        action *= (self.action_space.high - self.action_space.low)\n",
    "        action += self.action_space.low\n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        action -= self.action_space.low\n",
    "        action /= (self.action_space.high - self.action_space.low)\n",
    "        action = action * 2 - 1\n",
    "        return actions\n",
    "\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "arg4=g2apara()\n",
    "agent = REINFORCE(64,15+arg4.rnn_hidden_dim, gym.spaces.Discrete(7) ,arg4.n_agents,2)\n",
    "\n",
    "envs=[i[0] for i in envs0]\n",
    "obs_space, preprocess_obss=get_obss_preprocessor()\n",
    "#obs=envs[-1].reset()\n",
    "#obs['image']=np.zeros([7,7,3])\n",
    "#acmodel(preprocess_obss([obs]))[0].sample()\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    print('i_episode',i_episode)\n",
    "    visualise=True\n",
    "    rewardsRec=[[] for _ in range(arg4.n_agents)]\n",
    "    rewardcnt=[0 for i in range(arg4.n_agents)]\n",
    "    observations =  [envs[i].reset() for i in range(arg4.n_agents)]\n",
    "    entropies = [[] for _ in range(arg4.n_agents)]\n",
    "    log_probs = [[] for _ in range(arg4.n_agents)]\n",
    "    rewards = [[] for _ in range(arg4.n_agents)]\n",
    "    flag=[0 for _ in range(arg4.n_agents)]\n",
    "    \n",
    "\n",
    " #   action = action.cpu()\n",
    "    for i in range(arg4.n_agents):\n",
    "        print(i)\n",
    "        observations= [envs[i].reset() for i in range(arg4.n_agents)]\n",
    "        for t in range(10000):\n",
    "            action, log_prob, entropy = agent.select_action(observations,i)\n",
    "            observations[i], reward, done, _ = envs[i].step(action)\n",
    "            entropies[i].append(entropy)\n",
    "            log_probs[i].append(log_prob)\n",
    "            rewards[i].append(reward)\n",
    "            if reward:\n",
    "                print(i,reward)\n",
    "            if done:\n",
    "                print('over',t)\n",
    "                break\n",
    "    agent.update_parameters(rewards, log_probs, entropies, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
