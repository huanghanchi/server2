{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NodeAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "# taget_node: t_in_feature\n",
    "# other_node: o_in_feature\n",
    "    def __init__(self, t_in_features, o_in_features, out_features, nd_dropout, alpha, concat=True):\n",
    "        super(NodeAttentionLayer, self).__init__()\n",
    "        self.nd_dropout = nd_dropout\n",
    "        self.t_in_features = t_in_features\n",
    "        self.o_in_features = o_in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        \n",
    "        self.W_t = nn.Parameter(torch.zeros(size=(t_in_features, out_features)))\n",
    "        self.W_o = nn.Parameter(torch.zeros(size=(o_in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W_t.data, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.W_o.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, t_input, o_inout, adj):\n",
    "        #h_t = N_t*F\n",
    "        #h_o = N_o*F\n",
    "        h_t = torch.mm(t_input, self.W_t)\n",
    "        h_o = torch.mm(o_inout, self.W_o)\n",
    "        N_t = h_t.size()[0]\n",
    "        N_o = h_o.size()[0]\n",
    "        \n",
    "        a_input = torch.cat([h_t.repeat(1, N_o).view(N_t * N_o, -1), h_o.repeat(N_t, 1)], dim=1).view(N_t, N_o, 2 * self.out_features)\n",
    "        #e = N_t * N_o\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        # every slice along dim will sum to 1\n",
    "        # dim = 1 softmax in row\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.nd_dropout, training=self.training)\n",
    "        print(attention.shape,h_o.shape)\n",
    "        h_prime = torch.matmul(attention, h_o)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "class SchemaAttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, se_dropout, alpha):\n",
    "        super(SchemaAttentionLayer, self).__init__()\n",
    "        self.se_dropout = se_dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "#        self.b = nn.Parameter(torch.zeros(size=(1, out_features)))\n",
    "#        nn.init.xavier_uniform_(self.b.data, gain=1.414)\n",
    "        self.s = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.s.data, gain=1.414)\n",
    "        self.Tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "#input N*F + list of [N*F] \n",
    "    def forward(self, t_input, o_list):\n",
    "        N = t_input.size()[0]\n",
    "        h_t = torch.mm(t_input, self.W)\n",
    "#        print(h_t.size())\n",
    "        s_input = torch.cat([h_t,h_t],dim=1)\n",
    "        for h_o in o_list:\n",
    "            h_o = torch.mm(h_o, self.W)\n",
    "#            print(h_o.size())\n",
    "            temp = torch.cat([h_t,h_o],dim=1)\n",
    "            s_input = torch.cat([s_input,temp],dim=1)\n",
    "        #s_input = N*[(len(o_list)+1)*out_features]\n",
    "        s_input = s_input.view(3*N,2*self.out_features)\n",
    "        e = self.leakyrelu(torch.mm(s_input, self.s).view(N, -1))\n",
    "        schema_attentions = F.softmax(e, dim=1)  \n",
    "#        print(schema_attentions.size())\n",
    "#        print(schema_attentions[122,:])\n",
    "        #schema_attentions = N*P \n",
    "        schema_attentions = schema_attentions.unsqueeze(dim=1)\n",
    "         #schema_attentions = N*1*P \n",
    "        embed_list = []\n",
    "        embed_list.append(h_t)\n",
    "        for h_o in o_list:\n",
    "            h_o = torch.mm(h_o, self.W)\n",
    "            embed_list.append(h_o)\n",
    "        h_embedding = torch.cat(embed_list,dim = 1).view(N, -1 ,self.out_features)\n",
    "        #h_embedding = N*P*F\n",
    "        h_embedding = torch.matmul(schema_attentions, h_embedding).squeeze()\n",
    " \n",
    "        return h_embedding\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGAT(nn.Module):\n",
    "    def __init__(self, tfeat, nfeat_list, nhid, shid, nclass, nd_dropout, se_dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(HGAT, self).__init__()\n",
    "        self.nd_dropout = nd_dropout\n",
    "        self.se_dropout = se_dropout\n",
    "        self.nheads = nheads\n",
    "        self.node_level_attentions = []\n",
    "        for i in range(len(nfeat_list)):\n",
    "            self.node_level_attentions.append([NodeAttentionLayer(tfeat, nfeat_list[i], nhid, nd_dropout=nd_dropout, alpha=alpha, concat=True) for _ in range(nheads)])\n",
    "\n",
    "        for i, node_attentions_type in enumerate(self.node_level_attentions):\n",
    "            for j, node_attention in enumerate(node_attentions_type):\n",
    "                self.add_module('attention_path_{}_head_{}'.format(i,j), node_attention)\n",
    "        self.W = nn.Parameter(torch.zeros(size=(tfeat, nhid*nheads)))\n",
    "        self.schema_level_attention = SchemaAttentionLayer(nhid*nheads, shid, se_dropout, alpha)\n",
    "        \n",
    "        self.linear_layer = nn.Linear(shid, nclass)\n",
    "        \n",
    "    def forward(self, x_list, adjs):\n",
    "        x = x_list[0]\n",
    "        o_list = []\n",
    "        for i in range(0,len(x_list)-1):\n",
    "#            o_x = torch.stack([att(x, x_list[i+1], adjs[i]) for att in self.node_level_attentions[i]]).sum(0) / self.nheads\n",
    "            o_x = torch.cat([att(x, x_list[i+1], adjs[i]) for att in self.node_level_attentions[i]], dim=1)\n",
    "            o_list.append(o_x)\n",
    "        x = torch.mm(x_list[0], self.W)  \n",
    "        x = F.dropout(x, self.se_dropout, training=self.training)\n",
    "        \n",
    "        x = self.schema_level_attention(x, o_list)\n",
    "        \n",
    "        x = self.linear_layer(x)\n",
    "        \n",
    "#        embeddings = x\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "import random\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def load_data(path=\"../data/graph data/\", dataset=\"fake news\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "    features = []\n",
    "    labels = []\n",
    "    with open(path+'news_feature_vector_3000.pickle', 'rb') as f:\n",
    "         feature = pickle.load(f)\n",
    "         feature = normalize_features(feature)\n",
    "         features.append(feature.todense())\n",
    "    f.close\n",
    "    with open(path+'creator_feature_vector_3109.pickle', 'rb') as f:\n",
    "         feature = pickle.load(f)\n",
    "         feature = normalize_features(feature)\n",
    "         features.append(feature.todense())\n",
    "    f.close\n",
    "    with open(path+'subject_feature_vector_191.pickle', 'rb') as f:\n",
    "         feature = pickle.load(f)\n",
    "         feature = normalize_features(feature)\n",
    "         features.append(feature.todense())\n",
    "    f.close\n",
    "#    features = sp.csr_matrix(features, dtype=np.float32) \n",
    "    \n",
    "    with open(path+'index_label_2_class.txt', 'r') as l:\n",
    "        lines = l.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(' ')\n",
    "            labels.append(int(line[1]))\n",
    "    l.close\n",
    "    labels = encode_onehot(labels)    \n",
    "\n",
    "    adjs = []\n",
    "    for adj_name in ['news_creator','news_subject']:\n",
    "        with open(path+'{}_adj.pickle'.format(adj_name), 'rb') as f:\n",
    "             adj = pickle.load(f) \n",
    "        f.close\n",
    "#        adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "        adjs.append(adj.todense())\n",
    "\n",
    "    original = range(13826)\n",
    "    idx_train = random.sample(original,2765)\n",
    "    original = list(set(original) ^ set(idx_train))\n",
    "    idx_val = random.sample(original,1000)\n",
    "    original = list(set(original) ^ set(idx_val))\n",
    "    idx_test = random.sample(original,2800)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    return adjs, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "\n",
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n",
    "\n",
    "\n",
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "def macro_f1(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    macro = metrics.f1_score(labels, preds, average='macro')  \n",
    "    return macro\n",
    "\n",
    "def micro_f1(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    micro = metrics.f1_score(labels, preds, average='micro')  \n",
    "    return micro\n",
    "\n",
    "def macro_precision(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    micro = metrics.precision_score(labels, preds, average='macro')  \n",
    "    return micro\n",
    "\n",
    "def macro_recall(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    micro = metrics.recall_score(labels, preds, average='macro')  \n",
    "    return micro\n",
    "\n",
    "def f1(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    macro = metrics.f1_score(labels, preds)  \n",
    "    return macro\n",
    "\n",
    "\n",
    "def precision(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    micro = metrics.precision_score(labels, preds)  \n",
    "    return micro\n",
    "\n",
    "def recall(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    labels = labels.to(torch.device(\"cpu\")).numpy()\n",
    "    preds = preds.to(torch.device(\"cpu\")).numpy()\n",
    "    micro = metrics.recall_score(labels, preds)  \n",
    "    return micro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fake news dataset...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/graph data/news_feature_vector_3000.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-6c70dabf4c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0madjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Model and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-4376fe12d647>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path, dataset)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'news_feature_vector_3000.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m          \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m          \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/graph data/news_feature_vector_3000.pickle'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "# Training settings\n",
    "class para:\n",
    "    def __init__(self):\n",
    "        self.no_cuda=False\n",
    "        self.fastmode=False\n",
    "        self.sparse=False\n",
    "        self.seed=72\n",
    "        self.epochs=10000\n",
    "        self.lr=0.01\n",
    "        self.weight_decay=5e-4\n",
    "        self.nhidden=10\n",
    "        self.shidden=8\n",
    "        self.nb_heads=1\n",
    "        self.nd_dropout=0.4\n",
    "        self.se_dropout=0.0\n",
    "        self.alpha=0.1\n",
    "        self.patience=10\n",
    "\n",
    "args = para()\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "if torch.cuda.is_available():\n",
    "     device = torch.device(\"cuda\")\n",
    "     torch.cuda.set_device(0)\n",
    "else:\n",
    "     device = torch.device(\"cpu\")\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Load data\n",
    "adjs, features, labels, idx_train, idx_val, idx_test = load_data()\n",
    "\n",
    "# Model and optimizer\n",
    "t_feat = features[0].shape[1]\n",
    "nfeat_list = []\n",
    "for i in range(1,len(features)):\n",
    "    nfeat_list.append(features[i].shape[1])    \n",
    "\n",
    "#adjs = torch.FloatTensor(adjs)\n",
    "#features = torch.FloatTensor(features)\n",
    "for a in range(len(adjs)):\n",
    "    adjs[a] = torch.FloatTensor(adjs[a])\n",
    "    if args.cuda:\n",
    "        adjs[a] = adjs[a].cuda()\n",
    "for f in range(len(features)):\n",
    "    features[f] = torch.FloatTensor(features[f])  \n",
    "    if args.cuda:\n",
    "        features[f] = features[f].cuda()\n",
    "labels = torch.LongTensor(np.where(labels)[0])\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "model = HGAT(tfeat = t_feat,\n",
    "             nfeat_list=nfeat_list, \n",
    "             nhid=args.nhidden, \n",
    "             shid=args.shidden,\n",
    "             nclass=int(labels.max()) + 1, \n",
    "             nd_dropout=args.nd_dropout,\n",
    "             se_dropout=args.se_dropout,\n",
    "             nheads=args.nb_heads, \n",
    "             alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "#    features = features.cuda()\n",
    "#    adjs = adjs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "#features, adjs, labels = Variable(features), Variable(adjs), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adjs)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adjs)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.item(),loss_train.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adjs)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "#    mac = macro_f1(output[idx_test], labels[idx_test])  \n",
    "#    mac_pre = macro_precision(output[idx_test], labels[idx_test])  \n",
    "#    mac_rec = macro_recall(output[idx_test], labels[idx_test])  \n",
    "#    print(\"Test set results:\",\n",
    "#          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "#          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "#          \"macro_f1= {:.4f}\".format(mac),\n",
    "#          \"macro_precision= {:.4f}\".format(mac_pre),\n",
    "#          \"macro_recall= {:.4f}\".format(mac_rec))\n",
    "    mac = f1(output[idx_test], labels[idx_test])  \n",
    "    mac_pre = precision(output[idx_test], labels[idx_test])  \n",
    "    mac_rec = recall(output[idx_test], labels[idx_test])   \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "          \"macro_f1= {:.4f}\".format(mac),\n",
    "          \"macro_precision= {:.4f}\".format(mac_pre),\n",
    "          \"macro_recall= {:.4f}\".format(mac_rec))\n",
    "    \n",
    "# Train model\n",
    "print(\"start training!\")\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "loss_values_output = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch)[0])\n",
    "    loss_values_output.append(train(epoch)[1])\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "#for name, param in model.named_parameters():\n",
    "#    if param.requires_grad:\n",
    "#        print(name)\n",
    "\n",
    "# Testing\n",
    "compute_test()\n",
    "#print(len(loss_values_output))\n",
    "#print(loss_values_output)\n",
    "#print(len(loss_values))\n",
    "#print(loss_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfeat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-dfa6c8d9bb56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mloss_values_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-dfa6c8d9bb56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0macc_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-19e7393118ac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_list, adjs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#            o_x = torch.stack([att(x, x_list[i+1], adjs[i]) for att in self.node_level_attentions[i]]).sum(0) / self.nheads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mo_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_level_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mo_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-19e7393118ac>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#            o_x = torch.stack([att(x, x_list[i+1], adjs[i]) for att in self.node_level_attentions[i]]).sum(0) / self.nheads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mo_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_level_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mo_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-36a2adf3274e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, t_input, o_inout, adj)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mzero_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m9e15\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mattention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;31m# every slice along dim will sum to 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# dim = 1 softmax in row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "features=list(np.zeros([5,4,5]))\n",
    "adjs=list(np.zeros([5,4,5]))\n",
    "labels=list(np.zeros(5))\n",
    "labels[0]=1\n",
    "t_feat =5\n",
    "t_feat = features[0].shape[1]\n",
    "nfeat_list = []\n",
    "for i in range(1,len(features)):\n",
    "    nfeat_list.append(features[i].shape[1])    \n",
    "for a in range(len(adjs)):\n",
    "    adjs[a] = torch.FloatTensor(adjs[a])\n",
    "    if args.cuda:\n",
    "        adjs[a] = adjs[a].cuda()\n",
    "for f in range(len(features)):\n",
    "    features[f] = torch.FloatTensor(features[f])  \n",
    "    if args.cuda:\n",
    "        features[f] = features[f].cuda()\n",
    "labels = torch.LongTensor(np.where(labels)[0])\n",
    "\n",
    "original = range(5)\n",
    "idx_train = random.sample(original,2)\n",
    "original = list(set(original) ^ set(idx_train))\n",
    "idx_val = random.sample(original,2)\n",
    "original = list(set(original) ^ set(idx_val))\n",
    "idx_test = random.sample(original,1)\n",
    "\n",
    "idx_train = torch.LongTensor(idx_train)\n",
    "idx_val = torch.LongTensor(idx_val)\n",
    "idx_test = torch.LongTensor(idx_test)\n",
    "model = HGAT(tfeat = t_feat,\n",
    "             nfeat_list=nfeat_list, \n",
    "             nhid=args.nhidden, \n",
    "             shid=args.shidden,\n",
    "             nclass=int(labels.max()) + 1, \n",
    "             nd_dropout=args.nd_dropout,\n",
    "             se_dropout=args.se_dropout,\n",
    "             nheads=args.nb_heads, \n",
    "             alpha=args.alpha)\n",
    "optimizer = optim.Adam(model.parameters(), \n",
    "                       lr=args.lr, \n",
    "                       weight_decay=args.weight_decay)\n",
    "\n",
    "if args.cuda:\n",
    "    model.cuda()\n",
    "#    features = features.cuda()\n",
    "#    adjs = adjs.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()\n",
    "\n",
    "#features, adjs, labels = Variable(features), Variable(adjs), Variable(labels)\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adjs)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adjs)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.item(),loss_train.item()\n",
    "\n",
    "\n",
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adjs)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "#    mac = macro_f1(output[idx_test], labels[idx_test])  \n",
    "#    mac_pre = macro_precision(output[idx_test], labels[idx_test])  \n",
    "#    mac_rec = macro_recall(output[idx_test], labels[idx_test])  \n",
    "#    print(\"Test set results:\",\n",
    "#          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "#          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "#          \"macro_f1= {:.4f}\".format(mac),\n",
    "#          \"macro_precision= {:.4f}\".format(mac_pre),\n",
    "#          \"macro_recall= {:.4f}\".format(mac_rec))\n",
    "    mac = f1(output[idx_test], labels[idx_test])  \n",
    "    mac_pre = precision(output[idx_test], labels[idx_test])  \n",
    "    mac_rec = recall(output[idx_test], labels[idx_test])   \n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "          \"macro_f1= {:.4f}\".format(mac),\n",
    "          \"macro_precision= {:.4f}\".format(mac_pre),\n",
    "          \"macro_recall= {:.4f}\".format(mac_rec))\n",
    "    \n",
    "# Train model\n",
    "print(\"start training!\")\n",
    "t_total = time.time()\n",
    "loss_values = []\n",
    "loss_values_output = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch)[0])\n",
    "    loss_values_output.append(train(epoch)[1])\n",
    "    torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "\n",
    "    files = glob.glob('*.pkl')\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split('.')[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "\n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
    "#for name, param in model.named_parameters():\n",
    "#    if param.requires_grad:\n",
    "#        print(name)\n",
    "\n",
    "# Testing\n",
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
